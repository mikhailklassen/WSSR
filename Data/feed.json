[
 {
  "title": "Four short links: 20 May 2019",
  "content": "Account Hygiene, Conversational AI Playbook, Unix Time Falsehoods, and Testing/Debugging Machine Learning\n\nBasic Account Hygiene to Prevent Hijacking (Google) -- SMS 2FA  blocked 100% of automated bots, 96% of bulk phishing attacks, and 76% of targeted attacks. On-device prompts, a more secure replacement for SMS, helped prevent 100% of automated bots, 99% of bulk phishing attacks and 90% of targeted attacks.\n\n\nConversational AI Playbook -- The detailed instructions, practical advice, and real-world examples provided here should empower developers to improve the quality and variety of conversational experiences of the coming months and years.\n\n\nFalsehoods Programmers Believe about Unix Time -- These three facts all seem eminently sensible and reasonable, right? Unix time is the number of seconds since 1 January 1970 00:00:00 UTC. If I wait exactly one second, Unix time advances by exactly one second. Unix time can never go backward. False, false, false.\n\n\nTesting and Debugging in Machine Learning (Google) -- Testing and debugging machine learning systems differs significantly from testing and debugging traditional software. This course describes how, starting from debugging your model all the way to monitoring your pipeline in production.\n\n\nContinue reading Four short links: 20 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BykEE599rz8/four-short-links-20-may-2019"
 },
 {
  "title": "Four short links: 17 May 2019",
  "content": "Productsec, Supply Chain Attack, Sparse Neural Networks, and the Christchurch Call\n\nSix Buckets of Productsec -- There are six buckets a security bug can fall into on its journey through life: Prevented\u2014best outcome, never turned into code. Found automatically\u2014found via static analysis or other tools, \u201ccheap\u201d time cost. Found manually\u2014good even if it took more time; a large set of bugs can only be found this way. Found externally\u2014usually via bug bounty, put users at real risk, expensive time cost but 100x better than other outcomes. Never found\u2014most bugs probably end up here. Exploited\u2014the worst.\n\n\nShadowHammer (Bruce Schneier) -- The common thread through all of the above-mentioned cases is that attackers got valid certificates and compromised their victims\u2019 development environments. (via Bruce Schneier)\n\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks  -- dense, randomly initialized, feed-forward networks contain subnetworks (\"winning tickets\") that\u2014when trained in isolation\u2014reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\n\nChristchurch Call -- first time governments and companies have, en masse, sat at a table to figure out how to curb violent extremist content on the platforms.\n\nContinue reading Four short links: 17 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VKiiU_Sdd-M/four-short-links-17-may-2019"
 },
 {
  "title": "Four short links: 16 May 2019",
  "content": "Regulating Platforms, Amazon Development, Still Love Tech, and ML Cheatsheets\n\nThe Platform Challenge (Alex Stamos) -- absolute cracker of a talk about regulating the social media platforms. Must watch.\n\nAmazon's Away Teams -- Capturing the way things are at an organization as large as Amazon is always a challenge. The company has never publicly codified its management system as it has done for its leadership principles. But this picture might offer new ideas for people seeking to coordinate technology development at scale. (via Simon Willison)\n\nWhy I Still Love Tech (Wired) -- I love the whole made world. But I can\u2019t deny that the miracle is over, and that there is an unbelievable amount of work left for us to do.\n\n\nIllustrated Machine Learning Cheatsheets -- what it says on the box.\n\nContinue reading Four short links: 16 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xCZ5aB--UtQ/four-short-links-16-may-2019"
 },
 {
  "title": "The topics to watch in software architecture",
  "content": "Microservices, serverless, AI, ML, and Kubernetes are among the most notable topics in our analysis of proposals from the O\u2019Reilly Software Architecture Conference.The speaker proposals we receive for the O\u2019Reilly Software Architecture Conference are valuable because they come from speakers who are often the leading names in their fields. These go-to experts and practitioners operate on the front lines of technology. They also understand that business and architecture can no longer be compartmentalized, and that revenue is at stake.\n\nOur recent analysis[1] of speaker proposals from the O\u2019Reilly Software Architecture Conference turned up a number of interesting findings:Continue reading The topics to watch in software architecture.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/uEqL6rQDR9w/the-topics-to-watch-in-software-architecture"
 },
 {
  "title": "Four short links: 15 May 2019",
  "content": "Privacy, Decision Trees, Other People's Problems, and Programming Tools\n\nSenate Testimony (Maciej Ceglowski) -- This is an HTMLized version of written testimony I provided on May 7, 2019, to the Senate Committee on Banking, Housing, and Urban Affairs for their hearing on Privacy Rights and Data Collection in a Digital Economy. [...] The leading ad networks in the European Union have chosen to respond to the GDPR by stitching together a sort of Frankenstein\u2019s monster of consent, a mechanism whereby a user wishing to visit, say, a weather forecast page is first prompted to agree to share data with a consortium of 119 entities, including the aptly named \u201cA Million Ads\u201d network. The user can scroll through this list of intermediaries one by one, or give or withhold consent en bloc, but either way she must wait a further two minutes for the consent collection process to terminate before she is allowed to find out whether or not it is going to rain.\n\n\nAwesome Decision Tree Papers -- A collection of research papers on decision, classification, and regression trees with implementations.\n\n\nOther People's Problems (Camille Fournier) -- There\u2019s always going to be something you can\u2019t fix. So how do you decide where to exert your energy? Step one: figure out who owns this problem.\n\n\nToward the Next Generation of Programming Tools (Mike Loukides) -- one of the most interesting research areas in artificial intelligence is the ability to generate code.\n\n\nContinue reading Four short links: 15 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2b4-V6bt2Hc/four-short-links-15-may-2019"
 },
 {
  "title": "How AI and machine learning are improving customer experience",
  "content": "From data quality to personalization, to customer acquisition and retention, and beyond, AI and ML will shape the customer experience of the future.What can artificial intelligence (AI) and machine learning (ML) do to improve customer experience? AI and ML already have been intimately involved in online shopping since, well, the beginning of online shopping. You can\u2019t use Amazon or any other shopping service without getting recommendations, which are often personalized based on the vendor\u2019s understanding of your traits: your purchase history, your browsing history, and possibly much more. Amazon and other online businesses would love to invent a digital version of the (possibly mythical) sales person who knows you and your tastes, and can unerringly guide you to products you will enjoy.\n\n\n\nEverything begins with better data\n\n To make that vision a reality, we need to start with some heavy lifting on the back end. Who are your customers? Do you really know who they are? All customers leave behind a data trail, but that data trail is a series of fragments, and it\u2019s hard to relate those fragments to each other. If one customer has multiple accounts, can you tell? If a customer has separate accounts for business and personal use, can you link them? And if an organization uses many different names (we remember a presentation in which someone talked of the hundreds of names\u2014literally\u2014that resolved to IBM), can you discover the single organization responsible for them? Customer experience starts with knowing exactly who your customers are and how they\u2019re related. Scrubbing your customer lists to eliminate duplicates is called entity resolution; it used to be the domain of large companies that could afford substantial data teams. We\u2019re now seeing the democratization of entity resolution: there are now startups that provide entity resolution software and services that are appropriate for small to mid-sized organizations.\n\n\n\nOnce you\u2019ve found out who your customers are, you have to ask how well you know them. Getting a holistic view of a customer\u2019s activities is central to understanding their needs. What data do you have about them, and how do you use it? ML and AI are now being used as tools in data gathering: in processing the data streams that come from sensors, apps, and other sources. Gathering customer data can be intrusive and ethically questionable; as you build your understanding of your customers, make sure you have their consent and that you aren\u2019t compromising their privacy.\n\n\n\nML isn\u2019t fundamentally different from any other kind of computing: the rule \u201cgarbage in, garbage out\u201d still applies. If your training data is low quality, your results will be poor. As the number of data sources grows, the number of potential data fields and variables increases, along with the potential for error: transcription errors, typographic errors, and so on. In the past it might have been possible to manually correct and repair data, but correcting data manually is an error-prone and tedious task that continues to occupy most data scientists. As with entity resolution, data quality and data repair have been the subject of recent research, and a new set of machine learning tools for automating data cleaning are beginning to appear.\n\n\n\nApplications\n\nOne common application of machine learning and AI to customer experience is in personalization and recommendation systems. In recent years, hybrid recommender systems\u2014applications that combine multiple recommender strategies\u2014have become much more common. Many hybrid recommenders rely on many different sources and large amounts of data, and deep learning models are frequently part of such systems. While it\u2019s common for recommendations to be based on models that are only retrained periodically, advanced recommendation and personalization systems will need to be real time. Using reinforcement learning, online learning, and bandit algorithms, companies are beginning to build recommendation systems that constantly train models against live data.\n\n\n\nMachine learning and AI are automating many different enterprise tasks and workflows, including customer interactions. We\u2019ve all experienced chatbots that automate various aspects of customer service. So far, chatbots are more annoying than helpful\u2014though, well-designed, simple \u201cfrequently asked question\u201d bots can lead to good customer acquisition rates. But we\u2019re only in the early stages of natural language processing and understanding\u2014and in the past year, we\u2019ve seen many breakthroughs. As our ability to build sophisticated language models improves, we will see chatbots progress through a number of stages: from providing notifications, to managing simple question and answer scenarios, to understanding context and participating in simple dialogs, and finally to personal assistants that are \u201caware\u201d of their users\u2019 needs. As chatbots improve, we expect them to become an integral part of customer service, not merely an annoyance that you have to work through to get to a human. And for chatbots to reach this level of performance, they will need to incorporate real-time recommendation and personalization. They will need to understand customers as well as a human.\n\n\n\nFraud detection is another technology that is now digesting machine learning. Fraud detection is engaged in a constant battle between the good guys and the criminals, and the stakes are constantly increasing. Fraud artists are inventing more sophisticated techniques for online crime. Fraud is no longer person-to-person: it is automated, as in a bot that buys up all the tickets to events so they can be resold by scalpers. As we\u2019ve seen in many recent elections, it is easy for criminals to penetrate social media by creating a bot that floods conversations with automated responses. It is much harder to discover those bots and block them in real time. That\u2019s only possible with machine learning, and even then, it\u2019s a difficult problem that\u2019s only partially solved. But solving it is a critical part of re-building an online world in which people feel safe and respected.\n\n\n\nAdvances in speech technologies and emotion detection will reduce friction in automated customer interactions even further. Multi-modal models that combine different kinds of inputs (audio, text, vision) will make it easier to respond to customers appropriately; customers might be able to show you what they want or send a live video of a problem they\u2019re facing. While interactions between humans and robots frequently place users in the creepy \u201cuncanny valley,\u201d it\u2019s a safe bet that customers of the future will be more comfortable with robots than we are now.\n\n\n\nBut if we\u2019re going to get customers through to the other side of the uncanny valley, we also have to respect what they value. AI and ML applications that affect customers will have to respect privacy; they will have to be secure; and they will have to be fair and unbiased. None of these challenges are simple, but technology won\u2019t improve customer experience if customers end up feeling abused. The result may be more efficient, but that\u2019s a bad tradeoff.\n\n\n\nWhat will machine learning and artificial intelligence do for customer experience? It has already done much. But there\u2019s much more that it can do\u2014and that it has to do\u2014in building the frictionless customer experience of the future.\n\n\n\n\n\nRelated Content:\n\n\n\t\nProduct management in the machine learning era - a new tutorial at the Artificial Intelligence conference in San Jose\n\t\n\u201cMachine learning for personalization\u201d - Tony Jebara explains how Netflix is personalizing and optimizing the images shown to subscribers.\n\t\n\u201cReal-time entity resolution made accessible\u201d - Jeff Jonas on the evolution of entity resolution technologies\n\t\u201cThe next generation of AI assistants in enterprise\u201d\n\t\u201cSpecialized tools for machine learning development and model governance are becoming essential\u201d\n\t\u201cYou created a machine learning application. Now make sure it\u2019s secure.\u201d\n\t\u201cThe ethics of data flow\u201d\n\n\n\nContinue reading How AI and machine learning are improving customer experience.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HKVtdB2dTDU/how-ai-and-machine-learning-are-improving-customer-experience"
 },
 {
  "title": "Four short links: 14 May 2019",
  "content": "Designing for AI, Opinions, Data Moats, and Trans-inclusive Design\n\t\nPeople + AI Guidebook (Google) -- Designing human-centered AI products.\n\t\nStrong Opinions Loosely Held Might Be The Worst Idea in Tech -- What really happens? The loudest, most bombastic engineer states their case with certainty, and that shuts down discussion. Other people either assume the loudmouth knows best, or don\u2019t want to stick out their neck and risk criticism and shame. This is especially true if the loudmouth is senior, or there is any other power differential.\n\n\t\nThe Empty Promise of Data Moats (A16Z) -- Most data network effects are really scale effects.\n\n\t\nTrans-inclusive Design -- this is GOLD and should be required reading for every software designer and developer.\n\nContinue reading Four short links: 14 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9_z4bv0UEeQ/four-short-links-14-may-2019"
 },
 {
  "title": "Making Facebook a scapegoat is a mistake",
  "content": "Breaking up Facebook won't solve the disinformation or privacy problems. It might well make it harder for Facebook to work on those problems.Chris Hughes, a co-founder of Facebook, recently wrote an opinion piece for the New York Times presenting an argument for breaking up Facebook. I was trying to stay out of this discussion, and Nick Clegg's response pretty much summed up my opinions. But I got one too many emails from friends that simply assumed I'd be in agreement with Chris, especially given my critique of blitzscaling and Silicon Valley's obsession with \"world domination.\"\n\nIf Facebook should be broken up, it should be on grounds of anticompetitive behavior, and Chris barely even touches on that topic. Dragging in the history of all Facebook's various corporate missteps for which a breakup would provide no remedy just muddies the water and lets everyone think that punishment has been done. It's actually a way of NOT solving the actual problems. Breaking up Facebook won't solve the disinformation problem. It won't solve the privacy problem. It might well make it harder for Facebook to work on those problems.\n\nIn addition, if harm to our society is sufficient reason for a breakup, there are a lot of companies ahead of Facebook in the queue. Big banks. The pharma companies that brought us the opioid crisis. The energy cartel that's still fighting against responding to climate change 70 years after the warning bells were sounded.\n\nEven if you restrict yourself to surveillance capitalism, picking out one high-profile malefactor, issuing draconian punishment, and then going back to business as usual everywhere else is a lose-lose, not a win. Facebook is not at the root of this problem, nor even the worst offender. To solve this problem, we need to look not only beyond Facebook, but also beyond the entire tech industry. Banks and telcos are often worse privacy offenders than Google or Facebook. And if you restrict yourself to harm from disinformation, polarization, and radicalization, Twitter and YouTube are just as culpable and, out of the spotlight, appear to be doing less than Facebook to police their problems. Reddit is far worse, and traditional media outlets, especially the Fox News empire, almost as bad. (See Yochai Benkler's excellent book on Network Disinformation.)\n\nYes, Facebook has scale, but even there, I thought Chris' arguments were weak. In particular, I found the New York Times infographic to be quite disingenuous:\n\n\nFigure 1. Screenshot of the infographic on Chris Hughes' opinion piece in the New York Times.\n\n\nWhat's wrong with this infographic? It lists all of Facebook's properties, including those that are primarily messaging, while limiting the competition purely to social media, and failing to include other messaging products. Take out WhatsApp and Messenger, and Facebook suddenly doesn't look so big. Or add in other messaging products from companies like Apple, Google, Microsoft, and plain old SMS, and again, suddenly Facebook doesn't look so dominant. Add in Google's other properties besides YouTube, especially those like search and the new newsfeed on Android phones that influence what people see and understand about the world. Restrict it to the individual national markets where the competition actually happens, and it would look different again. This isn't analysis. This is polemics.\n\nI'm not saying there aren't grounds for investigation of anticompetitive behavior in the tech industry, or by Facebook specifically, but if someone's going to talk breakup, I'd love to see reporting and opinion pieces that make a true antitrust case, if there is one.\n\nIf the world really wants to tackle the problems that networked media creates or amplifies, making Facebook a scapegoat and letting everyone else off the hook is a massive mistake.\nContinue reading Making Facebook a scapegoat is a mistake.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VlqA-PLheXs/making-facebook-a-scapegoat-is-a-mistake"
 },
 {
  "title": "Four short links: 13 May 2019",
  "content": "Git-rebase, Swift on the Web, Deepfake Dal\u00ed, and ML Style Guide\n\nGit-rebase in Depth -- These tools can be a little bit intimidating to the novice or even intermediate git user, but this guide will help to demystify the powerful git-rebase.\n\n\nSwiftWasm -- Run Swift in browsers. SwiftWasm compiles your Swift code to WebAssembly.\n\n\nDeepfake Salvador Dal\u00ed Takes Selfies with Museum Visitors (Verge) -- Using archival footage from interviews, GS&P pulled over 6,000 frames and used 1,000 hours of machine learning to train the AI algorithm on Dal\u00ed\u2019s face. His facial expressions were then imposed over an actor with Dal\u00ed\u2019s body proportions, and quotes from his interviews and letters were synced with a voice actor who could mimic his unique accent, a mix of French, Spanish, and English. The selfie is magic, though. Prize to whoever thought that up!\n\nRules of ML (Google) -- a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine\u00ad-learned model, then you have the necessary background to read this document.\n\n\nContinue reading Four short links: 13 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/N1AnnuA1S8c/four-short-links-13-may-2019"
 },
 {
  "title": "Toward the next generation of programming tools",
  "content": "Programmers have built great tools for others. It\u2019s time they built some for themselves.In a Quora post, Alan Kay lamented the state of tooling for programmers. Every other engineering discipline has built modern computational tools: for computer aided design, simulation and testing, and for manufacturing. But programming hasn\u2019t progressed significantly since the 1970s. We\u2019ve built great tools for others, but not ourselves. The shoemaker\u2019s children have holes in their shoes.\n\nKay isn\u2019t being entirely fair, but before looking at what he\u2019s missing, it\u2019s important to think about how he\u2019s right. If we don\u2019t understand how he\u2019s right, we certainly won\u2019t understand what to build next, and where the future may be staring us in the face.\n\nLet\u2019s start with programming itself. We\u2019re still using punch cards, now emulated on modern high-resolution monitors. We\u2019re still doing line-oriented programming with an alpha-numeric character set. We\u2019re still using programming languages that, for the most part, behave like C or like LISP, and the biggest debates in the programming community are about which of these ancient paradigms is better. We have IDEs that make it somewhat easier to generate those virtual punch cards, but don\u2019t fundamentally change the nature of the beast. We have some tools for unit testing, but they work by requiring us to write more punch cards (unit tests). We have version control tools for managing changes to those punch cards. And we even have tools for continuous integration, continuous deployment, and container orchestration\u2014all of which are programmed by creating more virtual punch cards.\n\nDatabase developers are in somewhat better shape. Non-procedural languages like SQL lend themselves more readily to visual programming styles, yielding tools like Tableau, though those tools don't help much if you're connecting an enterprise application to a back-end database.\n\nWhere can we go from here? I\u2019ve long thought that the real next-generation programming language won\u2019t be a rehash of LISP, C, or Smalltalk syntax. It won\u2019t be character based at all: it will be visual. Rather than typing, we\u2019ll draw what we want. I\u2019ve yet to see a language that fits the bill. Teaching platforms like Alice and Scratch are an interesting attempt, but they don\u2019t go anywhere near far enough: they just take the programming languages we already know and apply a visual metaphor. A C-clamp instead of a loop. Plug-together blocks instead of keywords. Nothing has really changed.\n\nI suspect that the visual programming language we need will borrow ideas from all of our programming paradigms: it will pass messages, it will have objects, it will support concurrency, and so on. What it won\u2019t have is a textual representation; it won\u2019t be a visual sugarcoating to an underlying language like C.\n\nI have some ideas about where such a language might come from. I see two trends that might help us think about the future of programming.\n\nFirst, I see increasing evidence that there are two kinds of programmers: programmers who build things by connecting other things together, and programmers who create the things that others connect together. The first is the \u201cblue collar\u201d group of programmers; the second is the \u201cacademic\u201d group of programmers\u2013for example, the people doing new work in AI. Neither group is more valuable or important than the other. Building trades provide a good analogy. If I need to install a new kitchen sink, I call a plumber. He knows how to connect the sink to the rest of the plumbing; he doesn\u2019t know how to design the sink. There\u2019s a sink designer in an office somewhere who probably understands (in a rudimentary way) how to connect the sink to the plumbing, but whose real expertise is designing sinks, not installing them. You can\u2019t do without either, though the world needs more plumbers than designers. The same holds true for software: the number of people who build web applications is far greater than the number of people who build web frameworks like React or who design new algorithms and do fundamental research.\n\nShould the computational plumber use the same tools as the algorithm designer? I don\u2019t think so; I can imagine a language that is highly optimized for connecting pre-built operations, but not for building the operations themselves. You can see a glimpse of this in languages that apply functions to every element of a list: you no longer need loops. (Python\u2019s map() applies a function to every element of a list; there are languages where functions behave like this automatically.) You can see PHP as a language that\u2019s good for connecting web pages to databases, but horrible for implementing cryptographic algorithms. And perhaps this connective language might be visual: if we\u2019re just connecting a bunch of things, why not use lines rather than lines of code? (Mel Conway is working on \u201cwiring diagrams\u201d that allow consumers to build applications by describing interaction scenarios.)\n\nSecond, one of the most interesting research areas in artificial intelligence is the ability to generate code. A couple of years ago, we saw that AI was capable of optimizing database queries. Andrej Karpathy has argued that this ability places us on the edge of Software 2.0, in which AI will generate the algorithms we need. If, in the future, AI systems will be able to write our code, what kind of language will we need to describe the code we want? That language certainly won\u2019t be a language with loops and conditionals; nor do I think it will be a language based on the mathematical abstraction given by functions. Karpathy suggests that the core of this language will be tagged datasets for training AI models. Instead of writing step-by-step instructions, we will show the computer what we want it to do. If you think such a programming paradigm is too radical, too far away from the process of making a machine do your bidding, think about what the machine language programmers of the 1950s would think about a modern optimizing compiler.\n\nSo, while I can\u2019t yet imagine what a new visual language will look like, I can sense that we\u2019re on the edge of being able to build it. In fact, we\u2019re building it already. One of Jeremy Howard\u2019s projects at platform.ai is a system that allows subject matter experts to build machine learning applications without any traditional programming. And Microsoft has launched a\u00a0drag-and-drop machine learning tool that provides a graphical tool for assembling training data, cleaning it, and using it to build a model without any traditional programming. I suppose one could argue that this \u201cisn\u2019t real programming,\u201d but that\u2019s tantamount to defining programming by its dependence on archaic tools.\n\nWhat about the rest? What about the tools for building, testing, deploying, and managing software? Here\u2019s where Kay underestimates what we have. I\u2019d agree that it isn\u2019t much, but it\u2019s something; it\u2019s a foundation that we can build upon. We have more than 40 years of experience with build tools (starting with make in 1976), and similar experience with automated configuration (CFengine, the precursor to Chef and Puppet, goes back to the \u201890s), network monitoring (Nagios dates back to 2002), and continuous integration (Hudson, predecessor of Jeeves, dates to 2005). Kubernetes, which handles container orchestration, is the \u201cnew kid on the block.\u201d Kubernetes is the robotically automated factory for distributed systems. It\u2019s a tool for managing and operating large software deployments, running across many nodes. That\u2019s really a complete tool suite that runs from automated assembly through automated testing to fully automated production. Lights off; let the computers do the work.\n\nSadly, these tools are still configured with virtual punch cards (text files, usually in XML, YAML, JSON, or something equally unpleasant), and that\u2019s a problem that has to be overcome. I think the problem isn\u2019t difficulty\u2013creating a visual language for any of these tools strikes me as significantly easier than creating a visual language for programming\u2013it\u2019s tradition.\n\nSoftware people are used to bad tools. And while we\u2019d hate being forced to use physical punch cards (I\u2019ve done that, it\u2019s no fun), if you virtualize those punch cards, we\u2019re just fine. Perhaps it\u2019s a rite of passage, a sort of industrial hazing. \u201cWe survived this suckage, you should too if you\u2019re going to be a real programmer.\u201d We\u2019re happy with that.\n\nKay is right that we shouldn\u2019t be happy with this state of affairs. The pain of building software using tools that would be immediately understandable to developers in the 1960s keeps us thinking about the bits, rather than the meaning of those bits. As an industry, we need to get beyond that. We have prototypes for some of the tools we need. We just need to finish the job.\n\nWe need to imagine a different future for software development.\nContinue reading Toward the next generation of programming tools.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2TFrI3FzEKA/toward-the-next-generation-of-programming-tools"
 },
 {
  "title": "Four short links: 10 May 2019",
  "content": "Flip Disc Display, Misinformation, Surveillance, and TCP/IP over Logs\n\t\nTetris on a Flip-Disc Display (YouTube) -- the update click is ridiculously satisfying. (via BoingBoing)\n\t\nAgnotology and Epistemological Fragmentation (danah boyd) -- In 1995, Robert Proctor and Iain Boal coined the term \u201cagnotology\u201d to describe the strategic and purposeful production of ignorance. [...] there\u2019s an increasing number of people who are propagating conspiracy theories or simply asking questions as a way of enabling and magnifying white supremacy. This is agnotology at work. Fascinating in the details of how the misinformers do their work online.\n\t\nReverse Engineering a Xinjiang Police Mass Surveillance App (Human Rights Watch) -- discovering the data (online) saved by the surveillance system. TechCrunch even shows the tables.\n\t\nTCP/IP Over Amazon Cloudwatch Logs -- Running in a standard Go process, Richard Linklayer tunnels IP packets over Amazon Cloudwatch Log Streams that follow a special naming convention\u200a\u2014\u200athe stream and log group names are just MAC addresses. A cute hack.\n\nContinue reading Four short links: 10 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ry0y1gp61cE/four-short-links-10-may-2019"
 },
 {
  "title": "Real-time entity resolution made accessible",
  "content": "The O\u2019Reilly Data Show Podcast: Jeff Jonas on the evolution of entity resolution technologies.In this episode of the Data Show, I spoke with Jeff Jonas, CEO, founder and chief scientist of Senzing, a startup focused on making real-time entity resolution technologies broadly accessible. He was previously a fellow and chief scientist of context computing at IBM. Entity resolution (ER) refers to techniques and tools for identifying and linking manifestations of the same entity/object/individual. Ironically, ER itself has many different names (e.g., record linkage, duplicate detection, object consolidation/reconciliation, etc.).\n\nER is an essential first step in many domains, including marketing (cleaning up databases), law enforcement (background checks and counterterrorism), and financial services and investing. Knowing exactly who your customers are is an important task for security, fraud detection, marketing, and personalization. The proliferation of data sources and services has made ER very challenging in the internet age. In addition, many applications now increasingly require near real-time entity resolution.Continue reading Real-time entity resolution made accessible.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hMgEyXxtHeU/real-time-entity-resolution-made-accessible"
 },
 {
  "title": "Four short links: 9 May 2019",
  "content": "Adversarial Examples, War Crimes, Open Source Firmware, and Better Questions\n\t\nAdversarial Examples Are Not Bugs, They Are Features -- Adversarial vulnerability is a direct result of our models\u2019 sensitivity to well-generalizing features in the data.\n\n\t\nTech Companies Are Deleting Evidence of War Crimes (The Atlantic) -- By piecing together information that becomes publicly accessible on social media and other sites, internet users can hold the perpetrators accountable\u2014that is, unless algorithms developed by the tech giants expunge the evidence first. Facebook's automatic content removal tech is removing evidence these investigators use to hold war criminals to account. We live in an age when software designed to get college students laid is critical to prosecuting war criminals.\n\t\nWhy Open Source Firmware is Important for Security (Jessie Frazelle) -- It\u2019s counter-intuitive that the code that we have the least visibility into has the most privileges. This is what open source firmware is aiming to fix.\n\n\t\nTukey, Design Thinking, and Better Questions (Roger Peng) -- In my view, the most useful thing a data scientist can do is to devote serious effort towards improving the quality and sharpness of the question being asked. In my experience as well.\n\nContinue reading Four short links: 9 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ge93W_hnp1c/four-short-links-9-may-2019"
 },
 {
  "title": "Four short links: 8 May 2019",
  "content": "Old Timers, Web Flashback, Software Collapse, Revisions to Paxos\n\nBrian Kernighan interviews Ken Thompson (YouTube) -- wonderful footage from Vintage Computer Festival East 2019.\n\nHypertext and our Collective Destiny (Tim Berners-Lee) -- a 1995 talk honouring Vannevar Bush. I had (and still have) a dream that the web could be less of a television channel and more of an interactive sea of shared knowledge. (via Daniel G. Siegel)\n\nDealing with Software Collapse -- The main issue with the rot metaphor is that it puts the blame on the wrong piece of the puzzle. If software becomes unusable over time, it's not because of any alteration to that software that needs to be reversed. Rather, it's the foundation on which the software has been built, ranging from the actual hardware via the operating system to programming languages and libraries, that has changed so much that the software is no longer compatible with it.\n\n\nDistributed Consensus Revised (Part II) (The Morning Paper) -- In today\u2019s post, we\u2019re going to be looking at Chapter 3 of Dr. Howard\u2019s thesis, which is a tour (\u201csystematization of knowledge,\u201d SoK) of some of the major known revisions to the classic Paxos algorithm.\n\n\nContinue reading Four short links: 8 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lL1z471XB7w/four-short-links-8-may-2019"
 },
 {
  "title": "Four short links: 7 May 2019",
  "content": "AI in Dev Tools, Reproducibility, Computing Space, and Social Media and Free Speech\n\nMicrosoft's AI-Assisted Coding (TechCrunch) -- What makes IntelliCode different is that the company trained it by feeding it the code of thousands of open source projects from GitHub that have at least 100 stars. Using this data, the tool can then make smarter code-completion suggestions. It also takes the current code and context into account as it makes its recommendations. The first of (hopefully) many AI tools for coders. Interestingly, AI-style centralized training on large data sets isn't something that's a natural advantage for open source tools, so I wonder whether this marks the start of a dev tools shift in power to Microsoft.\n\nReproducibility as a Vehicle for Engineering Best Practices (Joel Grus) -- a talk aimed at machine learning folks, on how delivering reproducibility requires you to use better (more modern) development practices.\n\nDefining the Dimensions of the Space of Computing (JoDS) -- Glass rectangles and black cylinders are not the future. We can imagine other possible futures\u2014paths not taken\u2014by searching within a \u201cspace of alternative\u201d computing systems, as Simon has suggested. In this \u201cspace,\u201d even though some dimensions are currently less recognizable than others, by investigating and hence illuminating the less-explored dimensions together, we can co-create alternative futures. (via Daniel G. Siegel)\n\nOn Social Media (Tom Coates) -- Tom's Twitter thread about what's already filtered out in social media platforms in order to make it clear how fundamental it is that platforms filter content online.\n\n\nContinue reading Four short links: 7 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/tmmWbOj9Fqc/four-short-links-7-may-2019"
 },
 {
  "title": "Four short links: 6 May 2019",
  "content": "Intercepting App's Network Traffic, Rules Engine, CC Search, and Pronouncing Names\n\nPrivacy International's Data Interception Environment -- an emulator + MITM networking so you can spy on what apps are saying.\n\nJess -- an open source rules engine from Sandia. Jess uses an enhanced version of the Rete algorithm to process rules. Rete is a very efficient mechanism for solving the difficult many-to-many matching problem. [...] Jess has many unique features, including backward chaining and working memory queries, and of course Jess can directly manipulate and reason about Java objects. Jess is also a powerful Java scripting environment, from which you can create Java objects, call Java methods, and implement Java interfaces without compiling any Java code.\n\n\nCreative Commons Search Engine -- finally a good search for CC-licensed images.\n\nPronounce Like a Polyglot: A Guide to Saying Foreign Names (NPR) -- pronouncing someone's name correctly is a basic respect that you should accord them. Failure to do so will make everything else harder. (\"Foreign\" here is from an American perspective.)\n\nContinue reading Four short links: 6 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/peLF9HNZrgA/four-short-links-6-may-2019"
 },
 {
  "title": "Four short links: 3 May 2019",
  "content": "Knowledge Graph, Volumetric Viewer, Neutral Painting, Weird Codes\n\nBeam -- eBay's open source distributed knowledge graph store. (via eBay tech blog)\n\nNeuroglancer -- Neuroglancer is a WebGL-based viewer for volumetric data. It is capable of displaying arbitrary (non axis-aligned) cross-sectional views of volumetric data, as well as 3-D meshes and line-segment based models (skeletons). This is not an official Google product. Open source. A live demo is hosted here.\n\nNeural Painters -- We explore neural painters, a generative model for brushstrokes learned from a real non-differentiable and non-deterministic painting program. [...] Finally, we present a new concept called intrinsic style transfer. By minimizing only the content loss from neural style transfer, we allow the artistic medium, in this case, brushstrokes, to naturally dictate the resulting style. Source available (in CoLab notebooks no less). (via Reiichiro Nakano)\n\nReal and Strange ICD-10 Codes (John D. Cook) -- Some of the ICD-10 codes are awfully specific, and bizarre. For example, V95.4: Unspecified spacecraft accident injuring occupant; V97.33XA: Sucked into jet engine, initial encounter; V97.33XD: Sucked into jet engine, subsequent encounter. Your reminder that no classification system survives contact with the weirdness of reality.\n\nContinue reading Four short links: 3 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4njaRBNCfSE/four-short-links-3-may-2019"
 },
 {
  "title": "Rise of the (advertising) machines",
  "content": "Mike Tidmarsh looks at how data and AI are radically reshaping the world of marketing communications.Continue reading Rise of the (advertising) machines.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/AmFAJ-8y-b0/rise-of-the-advertising-machines"
 },
 {
  "title": "Privacy, identity, and autonomy in the age of big data and AI",
  "content": "Sandra Wachter argues that a right to reasonable inferences could protect against new forms of discrimination.Continue reading Privacy, identity, and autonomy in the age of big data and AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/08nBuYnetOA/privacy-identity-and-autonomy-in-the-age-of-big-data-and-ai"
 },
 {
  "title": "Building data science capacity in your organization",
  "content": "Shingai Manjengwa shares insights from teaching data science to 300,000 online learners.Continue reading Building data science capacity in your organization.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/oB8JsSlMu4Y/building-data-science-capacity-in-your-organization"
 },
 {
  "title": "Combining creativity and analytics",
  "content": "David Boyle shares lessons on how analysts can harness data and creativity to build partnerships.Continue reading Combining creativity and analytics.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BFhZ-j8L-eM/combining-creativity-and-analytics"
 },
 {
  "title": "The unstoppable rise of white box data",
  "content": "Chris Taggart explains the benefits of \u201cwhite box data\u201d and outlines the structural shifts that are moving the data world toward this model.Continue reading The unstoppable rise of white box data.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9ubtLwsx9cM/the-unstoppable-rise-of-what-box-data"
 },
 {
  "title": "Four short links: 2 May 2019",
  "content": "Network Computation, Algorithmic Bias, Social Robotics, Single Founders Do Fine\n\nWhen Should the Network Be the Computer? -- Researchers have repurposed programmable network devices to place small amounts of application computation in the network, sometimes yielding orders-of-magnitude performance gains. At the same time, effectively using these devices requires careful use of limited resources and managing deployment challenges. This paper provides a framework for principled use of in-network processing. We provide a set of guidelines for building robust and deployable in-network primitives, along with a taxonomy to help identify which applications can benefit from in-network processing and what types of devices they should use.\n\n\nThe Myth of the Impartial Machine -- nifty exploration of algorithmic bias and where it comes from, with interactive data demos.\n\nWhat Can We Learn from Social Robotics Failures? (IEEE Spectrum) -- Long-term engagement is the holy grail, and the Gordian knot; We need artists; Embodiment does create emotional bonds; Design matters. This line got me: All-in-all, I predict that when designers will start their own social robotics companies and hire engineers, rather than the other way around, we will finally discover what the hidden need for home robots was in the first place.\n\n\nCofounders and Single Founders -- In this paper, we examine the implications of founding alone versus as a group by using a unique data set of crowdfunded companies that together generated approximately $358 million in total revenue. We show that companies started by solo founders survive longer than those started by teams. Further, organizations started by solo founders generate more revenue than organizations started by founder pairs, and do not perform significantly different than larger teams. This suggests that the taken-for-granted assumption among scholars that entrepreneurship is best performed by teams should be reevaluated, with implications for theories of team performance and entrepreneurial strategy.\n\n\nContinue reading Four short links: 2 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hnOYvcT-lGg/four-short-links-2-may-2019"
 },
 {
  "title": "Finding your North Star",
  "content": "Cait O\u2019Riordan discusses the North Star metric the Financial Times uses across the organization to drive subscriber growth.Continue reading Finding your North Star.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HanV8-E8Sxs/finding-your-north-star"
 },
 {
  "title": "Highlights from the Strata Data Conference in London 2019",
  "content": "Watch highlights from expert talks covering machine learning, predictive analytics, data regulation, and more.People from across the data world came together in London for the Strata Data Conference. Below you'll find links to highlights from the event.\n\nMaking the future\n\nJames Burke asks if we can use data and predictive analytics to take the guesswork out of prediction.\n\n\n\tWatch \"Making the future.\"\n\n\nFinding your North Star\n\nCait O\u2019Riordan discusses the North Star metric the Financial Times uses to drive subscriber growth.\n\n\n\tWatch \"Finding your North Star.\"\n\n\nMaking data science useful\n\nCassie Kozyrkov explains how organizations can extract more value from their data.\n\n\n\tWatch \"Making data science useful.\"\n\n\nSustaining machine learning in the enterprise\n\nDrawing insights from recent surveys, Ben Lorica analyzes important trends in machine learning.\n\n\n\tWatch \"Sustaining machine learning in the enterprise.\"\n\n\nPrivacy, identity, and autonomy in the age of big data and AI\n\nSandra Wachter argues that a right to reasonable inferences could protect against new forms of discrimination.\n\n\n\tWatch \"Privacy, identity, and autonomy in the age of big data and AI.\"\n\n\nBuilding data science capacity in your organization\n\nShingai Manjengwa shares insights from teaching data science to 300,000 online learners.\n\n\n\tWatch \"Building data science capacity in your organization.\"\n\n\nCombining creativity and analytics\n\nDavid Boyle shares lessons on how analysts can harness data and creativity to build partnerships.\n\n\n\tWatch \"Combining creativity and analytics.\"\n\n\nThe enterprise data cloud\n\nMick Hollison explains why hybrid and multi-cloud will help organizations capitalize on the potential of machine learning and AI.\n\n\n\tWatch \"The enterprise data cloud.\"\n\n\nRise of the (advertising) machines\n\nMike Tidmarsh looks at how data and AI are radically reshaping the world of marketing communications.\n\n\n\tWatch \"Rise of the (advertising) machines.\"\n\n\nThe unstoppable rise of white box data\n\nChris Taggart explains the benefits of white box data and outlines the structural shifts that are moving the data world toward this model.\n\n\n\tWatch \"The unstoppable rise of white box data.\"\n\n\nContinue reading Highlights from the Strata Data Conference in London 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/V-9fWus5Mdw/highlights-from-strata-london-2019"
 },
 {
  "title": "Making the future",
  "content": "James Burke asks if we can use data and predictive analytics to take the guesswork out of prediction.Continue reading Making the future.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YLky8-Krr28/making-the-future"
 },
 {
  "title": "Sustaining machine learning in the enterprise",
  "content": "Drawing insights from recent surveys, Ben Lorica analyzes important trends in machine learning.Continue reading Sustaining machine learning in the enterprise.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/yx_Paijtj6M/sustaining-machine-learning-in-the-enterprise"
 },
 {
  "title": "Making data science useful",
  "content": "Cassie Kozyrkov explains how organizations can extract more value from their data.Continue reading Making data science useful.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BoR5YOaEJXU/making-data-science-useful"
 },
 {
  "title": "The enterprise data cloud",
  "content": "Mick Hollison describes why hybrid and multi-cloud is the future for organizations that want to capitalize on machine learning and AI.Continue reading The enterprise data cloud.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ntefUtct70k/the-enterprise-data-cloud-strata-london-2019"
 },
 {
  "title": "160+ live online training courses opened for May and June",
  "content": "Get hands-on training in machine learning, blockchain, cloud native, PySpark, Kubernetes, and many other topics.Learn new topics and refine your skills with more than 160 new live online training courses we opened up for May and June on the O'Reilly online learning platform.\n\nAI and machine learning\n\nReal-Time Streaming Analytics and Algorithms for AI Applications, May 15\nInside Unsupervised Learning: Anomaly Detection using Dimensionality Reduction, June 4\nBeginning Machine Learning with Scikit-learn , June 5\nGetting started with Machine Learning , June 10\nAI for Product Managers, June 11\nDeep Learning with TensorFlow, June 12\nIntermediate Machine Learning with Scikit-learn, June 12\nHands-on Adversarial Machine Learning , June 13\nInside Unsupervised Learning: Group Segmentation using Clustering, June 13\nReinforcement Learning: Building Recommender Systems, June 17\nInside Unsupervised Learning: Feature Extraction using Autoencoders and Semi-Supervised Learning, June 17\nFundamentals of Machine Learning with AWS, June 19\nBuilding Machine Learning Models with AWS Sagemaker, June 20\nA Practical Introduction to Machine Learning , June 25\nInside Unsupervised Learning: Generative Models and Recommender Systems, June 26\nArtificial Intelligence: AI for Business, July 2\n\nBlockchain\n\nSpotlight on Cloud: The Hidden Costs of Kubernetes with Bridget Lane, June 6\nSpotlight on Innovation: Blockchain as a Service with Ed Featherston, June 12\nSpotlight on Data: Caching Big Data for Machine Learning at Uber with Zhenxiao Luo, June 17\nIntroducing Blockchain, June 21\nBlockchain and Cryptocurrency Essentials , June 27\nIntroduction to Distributed Ledger Technology for Enterprise, July 10\n\nBusiness\n\nSpotlight on Cloud: Becoming Cloud Native with Jon Collins, May 2\nCore Agile, May 9\nSpotlight on Data: Data as an Asset with Friederike Sch\u00fc\u00fcr and Jen van der Meer, May 20\nSpotlight on Learning from Failure: Solving Cryptocurrency\u2019s Volatility Crisis with Wayne Chang, May 28\nIntroduction to Strategic Thinking Skills , June 4\nFoundations of Microsoft Excel, June 5\nFundamentals of Learning: Learn faster and better using neuroscience, June 6\nSucceeding with Project Management , June 10\nIntroduction to Employee Performance Management, June 10\nIntroduction to Leadership Skills, June 11\nBuilding Your LinkedIn Network, June 11\n60 minutes to Better User Stories and Backlog Management, June 13\nProduct Management in 90 Minutes , June 14\n60 Minutes to Better Email, June 18\nDeveloping Your Coaching Skills, June 18\nManaging Team Conflict, June 18\nEmpathy at Work, June 18\nAgile for Everybody, June 19\n60 Minutes to Designing a Better PowerPoint Slide, June 24\nIntroduction to Critical Thinking, June 25\nFundamentals of Cognitive Biases, June 25\nApplying Critical Thinking, June 26\nSalary Negotiation Fundamentals, June 27\nManaging Your Manager, June 27\nYour first 30 days as a manager, July 1\nLeading Innovative Teams, July 2\nPython-Powered Excel, July 8\nUnlock Your Potential, July 9\n60 Minutes to Better Product Metrics , July 10\nBusiness Fundamentals, July 10\nCore Agile, July 10\nProduct Roadmaps From the Ground Up , July 11\nBuilding Resiliency, July 11\nIntroduction to Time Management Skills , July 12\n\nData science and data tools\n\nPractical Linux Command Line for Data Engineers and Analysts, May 20\nFirst Steps in Data Analysis, May 20\nInferential Statistics Using R, May 24\nData Analysis Paradigms in the Tidyverse , May 30\nData Visualization with Matplotlib and Seaborn, June 4\nApache Hadoop, Spark and Big Data Foundations, June 5\nGetting Started with PySpark, June 5\nIntroduction to DAX Using Power BI , June 7\nReal-time Data Foundations: Kafka, June 11\nSQL Fundamentals for Data , June 12-13\nReal-time Data Foundations: Spark, June 13\nIntroduction to Statistics for Data Analysis with Python, June 17\nIoT Fundamentals, June 17-18\nData Pipelining with Luigi and Spark, June 19\nVisualization and Presentation of Data , June 20\nReal-time data foundations: Flink, June 25\nFraud Analytics using Python, June 25\nManaging Enterprise Data Strategies with Hadoop, Spark, and Kafka, June 25\nHands-On Algorithmic Trading with Python , July 2\n\nDesign and product management\n\nFrom User Experience Designer to Digital Product Designer, June 5\n\nProgramming\n\nEssentials of JVM Threading, May 13\nPython Full Throttle with Paul Deitel, May 30\nJava Full Throttle with Paul Deitel: A Code-Intensive One-Day Course, June 3\nProgramming with Data: Foundations of Python and Pandas, June 4\nIntroduction to TypeScript Programming , June 6\nConcurrency in Python, June 7\nAdvanced SQL Series: Relational Division , June 10\nModern Java Exception Handling, June 10\nFoundational Data Science with R, June 10-11\nGetting Started with React.js, June 11\nAdvanced SQL Series: Proximal and Linear Interpolations, June 12\nBuilding Web Apps with Vue.js, June 12\nAdvanced TypeScript Programming, June 12\nRethinking REST: A Hands-on Guide to GraphQL and Queryable APIs, June 13\nGetting Started with Pandas, June 17\nPython Data Handling: A Deeper Dive, June 17\nGetting Started with Python 3 , June 17-18\nHands-on Introduction to Apache Hadoop and Spark Programming, June 17-18\nBasic Android Development, June 17-18\nMastering Pandas, June 18\nGetting Started with Node.js, June 19\nData Structures in Java, June 19\nKotlin Fundamentals, June 20\nApplied Cryptography with Python, June 20\nBash Shell Scripting in 4 Hours, June 20\nPython Full Throttle with Paul Deitel , June 24\nFunctional Programming in Java , June 25-26\nIntroduction to the Bash Shell, June 26\nIntroduction to Python Programming , June 27\nBeginner\u2019s Guide to Writing AWS Lambda Functions in Python, June 28\nIntroduction to the Go Programming Language, July 1\nMastering Python\u2019s Pytest, July 1\nDesign Patterns Boot Camp , July 1-2\nReactive Spring and Spring Boot, July 9\nWhat's New In Java, July 10\n\nSecurity\n\nLinux, Python, and Bash Scripting for Cybersecurity Professionals, June 3\nCybersecurity Offensive and Defensive Techniques in 3 Hours, June 4\nCertified Ethical Hacker (CEH) Crash Course, June 5-6\nCISSP Crash Course, June 12-13\nCISSP Certification Practice Questions and Exam Strategies, June 13\nExpert Transport Layer Security (TLS), June 13\nCCNA Security Crash Course , June 20-21\nIntroduction to Ethical Hacking and Penetration Testing, June 20-21\nIntense Introduction to Hacking Web Applications, June 27\n\nSystems engineering and operations\n\nIP Subnetting from Beginning to Mastery, May 8-9\nSystems Design for Site Reliability Engineers: How To Build A Reliable System in Three Hours, May 14\nPractical Software Design from Problem to Solution, May 17\nCloud Computing Governance, May 29\nAnsible in 4 Hours, June 3\nAWS CloudFormation Deep Dive, June 3-4\nAutomating with Ansible, June 6\nNetwork DevOps, June 6\nIntroduction to Istio, June 6\nAWS Design Fundamentals , June 10-11\nDeploying Container-Based Microservices on AWS, June 10-11\nPractical Docker, June 11\nCloud Computing on the Edge, June 11\nDesigning Serverless Architecture with AWS Lambda, June 11-12\nRed Hat Certified Engineer (RHCE) Crash Course, June 11-14\nExam AZ-103: Microsoft Azure Administrator Crash Course, June 12-13\nSoftware Architecture Foundations: Characteristics and Tradeoffs, June 14\nMicroservices Caching Strategies, June 17\nFrom developer to software architect, June 17-18\nSoftware Architecture by Example, June 18\nModern streaming architectures , June 18-19\nAWS core architecture concepts , June 18-19\nMoving to the Cloud: What Your Company Needs to Know, June 19\nDocker: Up and Running , June 19-20\nKubernetes Serverless with Knative , June 20\nGetting started with continuous integration , June 20\nAWS Monitoring Strategies , June 20-21\nMastering SELinux, June 21\nIstio on Kubernetes: Enter the Service Mesh , June 21\nContinuous Delivery with Jenkins and Docker, June 24\nKafka Fundamentals, June 24-25\nNext Level Git - Master Your Workflow , June 25\nGoogle Cloud Platform Professional Cloud Architect Certification Crash Course , June 25-26\nAutomating Architectural Governance Using Fitness Functions, June 26\nGit Fundamentals, June 26-27\nAWS Certified Developer Associate Crash Course, June 26-27\nArchitecture for Continuous Delivery , June 27\nAWS Account Setup Best Practices, June 27\nComparing Service-Based Architectures , July 1\nFrom Developer to Software Architect, July 1-2\nIntroduction to Docker Compose, July 9\nIntroduction to Knative, July 9\nBuilding Data APIs with GraphQL, July 9\nMicroservice Fundamentals, July 10\nBuilding a Deployment Pipeline with Jenkins 2, July 10-11\nMicroservice Collaboration, July 11\nContinue reading 160+ live online training courses opened for May and June.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/df1_62PR90w/160-live-online-training-courses-opened-for-may-and-june"
 },
 {
  "title": "Four short links: 1 May 2019",
  "content": "Intermediate Vim, Newsletter Numbers, Automating Assessment, and Financial Modeling\n\nIntermediate Vim -- a few tips to level up your editing skills from beginner to intermediate.\n\n\nNewsletters Can Be Profitable (Buzzfeed) -- Substack\u2019s 12 top-earning writers make an average of more than $160,000 each, the company told BuzzFeed News. And more than 40,000 people are paying for Substack newsletters today.\n\n\nFinal Draft to Assess Against Bechdel Test (NYT) -- In an update announced Thursday, Final Draft\u2014software that writers use to format scripts\u2014said it will now include a proprietary \u201cInclusivity Analysis\u201d feature, allowing filmmakers \u201cto quickly assign and measure the ethnicity, gender, age, disability, or any other definable trait of the characters,\u201d including race, the company said in a statement. It also will enable users to determine if a project passes the Bechdel Test, measuring whether two female characters speak to each other about anything other than a man. The faster people get feedback, the more effect it has. I expect this to have significant effect on scripts. (via Marginal Revolution)\n\nHow Pharmaceutical Industry Financial Modelers Think About Your Rare Disease -- Today in \"how to see with the eyes of a specialist.\" To me, the biggest lesson from playing with this model has been to observe just how profoundly the lag time and probability of success shape the financial picture of drug development.\n\nContinue reading Four short links: 1 May 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hud3gq1qi68/four-short-links-1-may-2019"
 },
 {
  "title": "Looking Back on the O\u2019Reilly Artificial Intelligence Conference",
  "content": "More than anything else, O'Reilly's AI Conference was about making the leap to AI 2.0.At the start of O'Reilly's Artificial Intelligence Conference in New York this year, Intel's Gadi Singer made a point that resonated through the conference: \"Machine learning and deep learning are being put to work now.\" They're no longer experimental; they're being put to use in key business applications. The only real question is what we're going to get out of it. Will we be able to put it to use effectively? Will we find appropriate uses for it?\n\nNow that AI is moving out of the laboratory and into offices and homes, a number of questions are more important than ever. What kinds of tools will make it easier to build AI and ML systems? How will we make AI safe for humans? And what kinds of systems will augment human capabilities, rather than replace them? In short, as Aleksander Madry said in his talk, we are now at AI 1.0. How do we get to AI 2.0?\n\nMadry emphasized the importance of making AI ready for us: secure, reliable, ethical, and understandable. It's easy to see the shortcomings of AI now. Madry showed how easy it was to turn a pig into a jet by adding noise, or to become a movie star by changing your glasses. Getting to the next step won't be easy: training models will probably become more difficult, and those models may be more complex. We might need even more training data than we need now; and currently, one of the biggest barriers to widespread use of AI is the lack of training data. But the work that it takes to get to AI 2.0 will benefit us. We'll never have AI systems that don't make mistakes; but mistakes will be fewer, and they'll be more like the mistakes that humans make, rather than mistakes that are nonsensical. No more flying pigs. And that commonality might make it easier for those systems to work alongside us.\n\nWe saw many new tools for building AI systems: tools designed to make building these systems easier, allowing subject experts to play a bigger role. Danielle Dean of Microsoft showed how they built a recommendation system for machine learning pipelines; it sampled the space of possible pipelines and made recommendations about which to try. This approach drastically reduced the \"trial and error\" loop that characterizes a lot of AI development.\n\nStanford's Chris R\u00e9 demonstrated Snorkel, an open source tool for automating the process of tagging training data. An AI system has three components: a model, training data, and hardware. Advanced hardware for building AI systems is getting faster and cheaper; it's becoming a commodity. So are models: systems like Dean's, or like Intel's Nauta, simplify and democratize the task of building models. Training data is the one component that stubbornly resists commoditization. Acquiring and labeling data is labor intensive. Researchers have used low-cost labor from Mechanical Turk (or grad students) to label data, or gathered pre-labeled data from online sites like Flickr. But those approaches won't work in industry. Can we use AI to eliminate most of the work of tagging and turn it into a relatively simple programming problem? It looks like we can; if R\u00e9 is right, Snorkel and tools like it are a big step toward AI 2.0.\n\nWe saw many glimpses of the future. Olga Troyanska showed how deep learning is helping to decode the most difficult parts of the human genome: the logic that controls gene expression and, hence, cell differentiation. There are many diseases we know are genetic; we just don't know what parts of the genome are responsible. We will only be able to diagnose and treat those diseases when we understand how the language of DNA works.\n\nCMU's Martial Hebert's lab is taking AI into the human world by building systems that can reason about intent. If we want robots that can assist people, they need to be able to understand and predict human behavior in real time. He demonstrated how an AI system can help a paralyzed person perform tasks that would otherwise be impossible\u2014but only by reasoning about intent. Without this understanding, without knowing the goal was to pick up something or to open a door, the system was only able to twitch uselessly. All of this reasoning has to happen in hard real time: an autonomous vehicle needs to be able to predict whether a person will stand on the curb or run into the street, and it needs to do so with enough time to apply the brakes if needed.\n\nAny conference on AI needs to recognize the extraordinary messes and problems that automation can create. Sean Gourley of Primer talked about the arms race in disinformation. In the past year, we've gained the ability to create realistic images of fake people, and we've made tremendous progress in generating realistic language. We won't be able to handle these growing threats without the assistance of AI. Andrew Zaldivar talked about work at Google Jigsaw that tries to detect online abuse and harassment. Kurt Mehmel from Dataiku talked about progress toward ethical artificial intelligence, a goal we will only reach if we build teams that are radically inclusive. The saying goes, \"with enough eyes, all bugs are shallow\"; but that's only true if those eyes are all different eyes, looking at problems in different ways. The solution isn't to build better technology; rather, it's making sure the people most likely to be impacted by the technology are included at all steps of the process.\n\nThe conference sessions covered everything from advanced AI techniques in reinforcement learning and natural language processing to business applications, to deploying AI applications at scale. AI is quickly moving beyond the hype: it's becoming part of the everyday working world. But as Aleksander Madry said, we need to make AI human-ready. We need to get to AI 2.0. More than anything else, O'Reilly's AI Conference was about making that leap.\nContinue reading Looking Back on the O\u2019Reilly Artificial Intelligence Conference.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/wjDz8_BLJjc/looking-back-on-the-oreilly-artificial-intelligence-conference"
 },
 {
  "title": "Four short links: 30 April 2019",
  "content": "AI Engines, Self-Grading Labs, Software Project Heroes, and Embedded Rust\n\nArtificial Intelligence Engines -- In this richly illustrated book, key neural network learning algorithms are explained informally first, followed by detailed mathematical analyses. (via Tom Stafford)\n\nCMU Self-Grading Labs -- an excellent next step in CS education. Feedback is more effective the closer to the moment of error it is. (via Hacker News)\n\nWhy Software Projects Need Heroes: Lessons Learned from 1,100+ Projects -- A \"hero\" project is one where 80% or more of the contributions are made by 20% of the developers. In the literature, such projects are deprecated since they might cause bottlenecks in development and communication. However, there is little empirical evidence on this matter. Further, recent studies show that such hero projects are very prevalent. Accordingly, this paper explores the effect of having heroes in project, from a code quality perspective. We identify the hero developer communities in 1100+ open source GitHub projects. Based on the analysis, we find that (a) hero projects are majorly all projects; and (b) the commits from \"hero developers\" (who contribute most to the code) result in far fewer bugs than other developers. That is, contrary to the literature, heroes are standard and a very useful part of modern open source projects. Extrapolation to your own software team is done at your own risk. As someone on Hacker News said, \"Of course, nearly every GitHub project is going to have heroes\u2014we call them \"maintainers.\"\n\nThe Embedded Rust Book -- An introductory book about using the Rust programming language on \"bare metal\" embedded systems, such as Microcontrollers.\n\n\nContinue reading Four short links: 30 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5stzvZH40TA/four-short-links-30-april-2019"
 },
 {
  "title": "How companies adopt and apply cloud native infrastructure",
  "content": "Survey results reveal the path organizations face as they integrate cloud native infrastructure and harness the full power of the cloud.\nDriven by the need for agility, scaling, and resiliency, organizations have spent more than a decade moving from \u201ctrying out the cloud\u201d to a deeper, more sustained commitment to the cloud, including adopting cloud native infrastructure. This shift is an important part of a trend we call the Next Architecture, with organizations embracing the combination of cloud, containers, orchestration, and microservices to meet customer expectations for availability, features, and performance.\n\nTo learn more about the motivations and challenges companies face adopting cloud native infrastructure, we conducted a survey of 590 practitioners, managers, and CxOs from across the globe.[1]\n\nKey findings from the survey include:\n\n\n\tNearly 50% of respondents cited lack of skills as the top challenge their organizations face in adopting cloud native infrastructure. Given the industry is both new and rapidly evolving, engineers struggle to keep up-to-date on new tools and technologies.\n\t40% of respondents use a hybrid cloud architecture. The hybrid approach can accommodate data that can\u2019t be on a public cloud, and can serve as an interim architecture for organizations migrating to a cloud native architecture.\n\t48% of respondents rely on a multi-cloud strategy that involves two or more vendors, helping organizations avoid lock-in to any one cloud provider and providing access to proprietary features that each of the major cloud vendors provide.\n\t47% of respondents working in organizations that have adopted cloud native said DevOps teams are responsible for their organizations\u2019 cloud native infrastructures, signaling a tight bond between DevOps and cloud native concepts.\n\tAmong respondents whose organizations have adopted cloud native infrastructure, 88% use containers and 69% use orchestration tools like Kubernetes. These signals align with the Next Architecture\u2019s hypothesis that cloud native infrastructure best meets the demands put on an organization\u2019s digital properties.\n\n\nIn our analysis, we assigned experience levels to our respondents for some of the survey questions. New respondents work at organizations that have been cloud native for less than one year; early respondents\u2019 organizations have been cloud native for one to three years; and sophisticated respondents work at organizations that have been cloud native for more than three years.\n\nWhat respondents do and where they work\n\n\nFigure 1. Roles of survey respondents.\n\n\nThe results in Figure 1 aren\u2019t surprising, given that more developers are making technology decisions. If you combine software practitioners (engineers, developers, and admins) with leads/architects, that\u2019s nearly 70% of respondents with technical roles. We see cloud native creating an increasing need for technical and architectural leadership, and more overlap between lead roles and engineering work. We expect those in technical roles to exert more influence over tool selection and development in the months and years ahead.\n\n\nFigure 2. Time in current role of survey respondents.\n\n\nAround half of respondents have been in their current position for three years or less (Figure 2). This points to the shifting nature of jobs as the industry responds to new technologies and workflows.\n\nCloud native has played a role in this shift. Important cloud native tools like Docker (released in 2013) and Kubernetes (released in 2015) are both relatively new. Containers, like Docker, and orchestration tools, like Kubernetes, are essential for creating the runway organizations need to transform their digital properties from monolith to microservice architectures\u2014a task many companies are still in the early days of performing. With 72% of respondents having adopted cloud native in the last three years (see Figure 9, below), we see the tools and infrastructure changing quickly, and people learning new skills on the job.\n\n\nFigure 3. Industry of survey respondents, broken down by cloud native experience level.\n\n\nSurvey respondents whose organizations adopted cloud native three-plus years ago\u2014referred to in this analysis as \"sophisticated\"\u2014typically work at software companies (Figure 3). Respondents in the finance and banking industry show the opposite pattern, with a larger share of those new to cloud native and a smaller share of sophisticated respondents. A legacy of outdated back office applications and regulatory and security concerns makes the migration to cloud native for those in finance that much more difficult. However, competitive pressure from fintech startups and increased competition from payment providers and pay services like Apple, Google, and Amazon create the imperative to transition to a cloud native infrastructure.\n\nMedia and entertainment companies show a notable share of experienced cloud native respondents, with nearly all falling into the early adopter or sophisticated categories. This isn\u2019t too surprising, given the essential role microservices play in that industry. Netflix is a good media company case study, with its early adoption of microservices, its contributions to chaos engineering, and its open source, multi-cloud continuous delivery platform Spinnaker (which Netflix and Google recently donated to the newly launched Continuous Delivery Foundation (CDF)). Cloud native adoption will likely increase in the media and entertainment space, as the distributed systems that make cord-cutting possible require cloud native infrastructures.\n\nWhich cloud providers and cloud types are most popular\n\n\nFigure 4. Types of cloud infrastructure used by survey respondents.\n\n\n\nFigure 5. Types of cloud infrastructure used by survey respondents, broken down by cloud native experience level.\n\n\nWhen asking about the types of cloud platforms respondents use (Figure 4), we define public cloud as services offered solely through a third-party provider; hybrid cloud as using a mix of private and third-party cloud services; and private cloud as a secure, on-premises cloud infrastructure with restricted access.\n\nMore than 40% of respondents indicate their organizations use a hybrid cloud infrastructure (Figure 4). We expect hybrid to remain a popular option for accommodating data that can\u2019t be stored on a public cloud and for serving as an interim architecture for organizations progressively transitioning their services and applications to a cloud native architecture\n\nA close read of the responses in Figure 5 shows the more sophisticated the respondent\u2019s cloud implementation, the more likely they are to use public cloud providers. Those new to the cloud are more likely to use hybrid cloud options. A smaller share of respondents report using only private cloud. This could be reflective of industries that can\u2019t put their data on a public cloud due to laws and regulations (finance and government, for example) and the complexity of migrating legacy systems.\n\n\nFigure 6. Public cloud providers used by survey respondents, broken down by cloud native experience level.\n\n\n\nFigure 7. Cloud provider share among survey respondents.\n\n\nAmazon AWS leads across experience levels (Figure 6), but we also see some interesting cloud provider combinations among respondents (Figure 7):\n\n\n\tAWS and Microsoft Azure (18%)\n\tAWS, Azure, and Google Cloud (15%)\n\tAWS and Google Cloud (14%)\n\n\nWith 48% of respondents having adopted a multi-cloud architecture (Figure 7), we see a competitive marketplace, with organizations sampling from a mix of vendors. We suspect organizations are \u201ckicking the tires\u201d on cloud providers, looking to test proprietary features or trying to lower costs by using more than one vendor. And, the strategy helps keep vendors honest, as containers allow an easy switch between platforms. Most organizations also want to maximize performance and resiliency, and a multi-cloud strategy allows for scaling and redundancy to help mitigate the risk of costly downtime and potential data loss in the case of an incident.\n\nHow organizations view their adoption of cloud native infrastructure\n\n\nFigure 8. Cloud native infrastructure adoption among survey respondents.\n\n\nWhile 68% of respondents said their organizations have adopted, or at least have begun to adopt, cloud native infrastructure, more than 30% of self-selecting respondents to a cloud native survey let us know they haven\u2019t adopted cloud native (Figure 8). The results show both the great interest in cloud native, and the great potential\u2014one of the reasons we expect cloud native to be of interest for many years to come. The cloud native ecosystem is not yet settled, and computing is not quite a utility, so we anticipate much change as trends around cloud native coalesce.\n\nThe charts that follow show results from the 68% of survey respondents whose organizations have adopted cloud native infrastructure (Figure 8).\n\n\nFigure 9. Cloud native adoption history among survey respondents.\n\n\nThe results in Figure 9 reflect the newness of the cloud native space, with 72% of respondents adopting cloud native in the last three years.\n\nNew attention to cloud native mirrors what we\u2019ve seen on the O\u2019Reilly online learning platform. Cloud native topics are among the fastest growing areas. In search and usage on the O\u2019Reilly platform, Kubernetes is the fastest growing large topic, with the major cloud vendors and other cloud native tools all growing strongly as well. While the cloud has been around for more than 10 years, the acceleration in interest we see on the platform shows the cloud an ascendent topic with strong legs and a good foundation for sustained growth. In particular, patterns around cloud native components helped drive support for the Next Architecture, as we note in this report.\n\n\nFigure 10. Rating of success of cloud native adoption among survey respondents, broken down by cloud native experience level.\n\n\nThe levels of success noted by sophisticated adopters reflect how experience with cloud native infrastructure pays off (Figure 10). Sophisticated respondents had by far the largest share of extreme success with their cloud native adoption. More than 90% of sophisticated respondents rated their cloud native implementations as mostly successful or better, and no sophisticated respondents felt their implementations were unsuccessful.\n\n\nFigure 11. Cloud native challenges faced by survey respondents.\n\n\n\nFigure 12. Cloud native challenges faced by survey respondents, broken down by cloud native experience level.\n\n\nAdopting a cloud native infrastructure is both complex and difficult, which is made clear from the survey results showing at least 40% of respondents citing challenges with finding skilled engineers, migrating from legacy architecture, responding to security and compliance demands, managing technical infrastructure, and transforming their corporate culture (Figure 11).\n\nThat lack of skills is a much bigger factor than hiring shows respondents are trying to adopt cloud native on their own\u2014a sign that organizations are fundamentally structuring themselves around cloud native architectures, not looking to hire that skill from outside. We see in Figure 12 that early respondents struggle significantly more with lack of skills and company culture than the other categories, suggesting these are issues organizations should consider tackling first when adopting cloud native.\n\nIn that same vein, as organizations restructure internal processes and pipelines to adopt a cloud native architecture, they struggle to put a corporate culture in place that supports this new way of working. Cloud native adoption isn\u2019t just about using the right tools or having the right technical infrastructure (though, those are important). Adopting a DevOps workflow that breaks down the barriers between teams and embraces a culture of collaboration is essential to implementing a successful cloud native architecture that relies on continuous integration, delivery, and improvement.\n\nAnd finally, security and compliance hurdles are not going away, even if they are less prevalent than a few years ago, when health data and financial regulations limited what many organizations could even consider placing in the cloud. While some of those hurdles have been resolved, our respondents tell us that security and compliance continue to require attention when considering cloud native implementations.\n\nThe teams and tools that manage cloud native infrastructure\n\n\nFigure 13. Teams that manage cloud native infrastructure among survey respondents.\n\n\n\nFigure 14. Teams that manage cloud native infrastructure among survey respondents, broken down by cloud native experience level.\n\n\nDevOps was by far the top choice for who manages the respondents\u2019 cloud native infrastructures (Figure 13). This is evidence that adopting a DevOps culture is critical to meeting the market demands\u2014agility, speed to market, scaling, and reliability\u2014that cloud native also addresses.\n\nIt\u2019s telling, though, when looking at the responses in the context of maturity, the more sophisticated cohort said they depend on site reliability engineering (SRE) to manage their cloud native infrastructure (Figure 14). SRE is a practice started at Google where engineers take on both development and operations responsibilities to release software faster and more reliably. SREs split their time between developing and maintaining infrastructure by automating repetitive tasks (toil) to allow time to build new features. While DevOps is a set of principles that loosely define how teams should work together to remove silos and collaborate effectively, SRE serves as an implementation of DevOps and is a job role that will see increased demand as organizations continue their transitions to cloud native.\n\n\nFigure 15. Cloud tools used by survey respondents.\n\n\n\nFigure 16. Cloud tools used by survey respondents, broken down by cloud native experience level.\n\n\nIt\u2019s no surprise that containers show as the most popular tool (Figure 15), as containers serve a key cloud native infrastructure role, improving software developer productivity, deployment speed and flexibility, platform independence, and enabling effective scaling. In addition, the high number of responses for orchestration and management underscores the strength of Kubernetes in this field. Orchestration and containers, combined with the cloud and microservices, form the technical foundation of the Next Architecture.\n\nThe percentages around service mesh and serverless were lower, likely because these technologies are relatively immature (Figure 15). However, it\u2019s worth briefly outlining how they work and why they\u2019re poised for adoption because we expect both tools to play large future roles in the cloud native space.\n\nA service mesh is a configurable, low-latency infrastructure layer designed to ease the complexity of networking microservices and managing communication between them in large, distributed networks. Istio, arguably the most popular service mesh, was developed by Google, Lyft, and IBM as an open source solution, and only reached version 1.0 in July 2018. We\u2019ll see more organizations turning to service mesh infrastructure as the tools mature and expanding organizations seek solutions for scaling their systems and managing the increasing complexity that comes with a global, distributed network.\n\nWhile service mesh provides networking infrastructure, serverless provides another layer of abstraction for cloud developers. Despite the name, serverless does not mean there are no servers. Serverless architecture simply puts the onus of managing backend infrastructure on the cloud provider so developers can focus on building applications rather than the software that powers them. Serverless architecture is generally stateless, provisions resources on demand, and only incurs cost for the resources actually used, potentially enabling organizations to scale rapidly while saving money. However, the technology still has significant drawbacks, as outlined in a recent study by the University of California at Berkeley, including inadequate storage, and performance and security concerns. But you can expect to see these issues resolved within the next decade. As serverless matures, Berkeley researchers predict it will \u201cbecome the default computing paradigm for the Cloud Era, largely replacing serverful computing and thereby bringing an end to the Cloud-Server Era.\u201d\n\nThe survey results also reveal a story around cloud native experience and tool adoption (Figure 16). Sophisticated cloud native organizations are in a position to iteratively improve and move into tools like service mesh and serverless. Organizations new to cloud native may need to wait and learn before they can fully harness some of these tools.\n\nWhy companies haven't adopted cloud native infrastructure\n\nThe charts in this section show results from the 32% of survey respondents whose organizations have not adopted cloud native infrastructure (see Figure 8, above).\n\n\nFigure 17. Reasons why survey respondents have not adopted cloud native.\n\n\nThe interesting story here is that the same challenges were identified whether respondents\u2019 organizations have or have not moved to cloud native (compare Figure 11 and Figure 17). Lack of skills, company culture, microservices migration, and security and compliance were shared challenges among adopters and non-adopters alike.\n\n\nFigure 18. When survey respondents whose organizations have not adopted cloud native infrastructure expect to implement cloud native.\n\n\nIn Figure 18, it\u2019s surprising to see 27% of respondents work at organizations with no plans for cloud native adoption. While we can\u2019t determine exactly why they have no plans, it seems prudent to investigate cloud native even if the adoption timeline for your organization will be long. This result may also reflect the newness of the space, as some organizations might not yet realize how important cloud native is and will be.\n\nRecommendations and concluding thoughts\n\nWhen evaluating responses from the sophisticated cohort, a few lessons emerge for organizations considering cloud native and those that are in the early stages of cloud native implementations:\n\n\n\tCloud native success comes empirically. Don\u2019t try to overhaul your entire architecture at once. Start small and focus on high-impact services that show clear value to build internal support for investing in the ongoing transition.\n\tIt\u2019s best to manage expectations. Focus on learning from, and building on, your early cloud native efforts.\n\tTake advantage of training opportunities, including social learning via conferences where you can gain access to best practices from those with the most cloud native experience.\n\n\nMore generally, as companies rise to meet the increasingly real-time demands of users and customers, and the need to respond to nimble competitors, we expect organizations to find the cloud native approach a necessity. Cloud native makes it possible for companies to deploy features faster, more affordably, more reliably, and with less risk. As the cloud native market develops, we see many opportunities for tools and training to help ease the transition to new architectures and to bridge the cloud native skills gap.\n\n\n\n[1] The cloud native infrastructure adoption survey ran from February 27-March 27, 2019.\n\n\n\nContinue reading How companies adopt and apply cloud native infrastructure.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-jc2XqcPwso/how-companies-adopt-and-apply-cloud-native-infrastructure"
 },
 {
  "title": "Four short links: 29 April 2019",
  "content": "Technology Radar, Influencers Dropping, Stuff That Matters, and Reverse Engineering\n\nThoughtworks Technology Radar (PDF) -- an interesting rating system for a bunch of different tools, techniques, platforms, and languages/frameworks.\n\nInfluencers are Abandoning the Instagram Look (The Atlantic) --  According to Fohr, 60% of influencers in his network with more than 100,000 followers are actually losing followers month over month. \u201cIt\u2019s pretty staggering,\u201d he says. \u201cIf you\u2019re an influencer [in 2019] who is still standing in front of Instagram walls, it\u2019s hard.\u201d\n\n\nLunch with Alan Kay -- What it comes down to is: are you trying to do science? Are you trying to invent a good future for humanity? Alan\u2019s definition of science is still too large to fit into my head, but I can see his reverence for it and the pioneering scientists of our past. [...] For me this lunch felt like a reckoning. It was as if (to be clear: this didn\u2019t really happen), Alan clapped his hands loudly in my face, shouting \u201cWake up! Wake up!\u201d, and then turned me away from the flame everyone else was transfixed by and onto a helicopter ride to give me a glimpse of all the other perspectives that I should consider.\n\n\nJane Manchun Wong -- writeups of details of app features, using knowledge gained by reverse engineering those apps. Fascinating!\n\nContinue reading Four short links: 29 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zYpKS1yCcf8/four-short-links-29-april-2019"
 },
 {
  "title": "Four short links: 26 April 2019",
  "content": "Simplify Gmail, Watch Web Pages, Easy Debugging, and Sleep Deprivation\n\nSimplify -- A Chrome extension that brings the simplicity of Google Inbox to Gmail. (via FastCompany)\n\nWatchMe -- WatchMe can watch for changes to an entire page, or a specific section of it. It's appropriate for research use cases where you want to track changes in one or more pages over time. WatchMe also comes with psutils [Python system and process utilities] (system tasks) built in to allow for monitoring of system resources. Importantly, it is a tool that implements reproducible monitoring, as all your watches are stored in a configuration file that can easily be shared with others to reproduce your watching protocol.\n\n\nPySnooper -- instead of carefully crafting the right print lines, you just add one decorator line to the function you're interested in. You'll get a play-by-play log of your function, including which lines ran and when, and exactly when local variables were changed.\n\n\nNeed for Sleep -- we found that a single night of sleep deprivation leads to a reduction of 50% in the quality of the implementations. There is notable evidence that the developers\u2019 engagement and their prowess to apply TFD [test-first development] are negatively impacted. Our results also show that sleep-deprived developers make more fixes to syntactic mistakes in the source code.\n\n\nContinue reading Four short links: 26 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/XqU1kvh8Bzw/four-short-links-26-april-2019"
 },
 {
  "title": "Why companies are in need of data lineage solutions",
  "content": "The O\u2019Reilly Data Show Podcast: Neelesh Salian on data lineage, data governance, and evolving data platforms.In this episode of the Data Show, I spoke with Neelesh Salian, software engineer at Stitch Fix, a company that combines machine learning and human expertise to personalize shopping. As companies integrate machine learning into their products and systems, there are important foundational technologies that come into play. This shouldn\u2019t come as a shock, as current machine learning and AI technologies require large amounts of data\u2014specifically, labeled data for training models. There are also many other considerations\u2014including security, privacy, reliability/safety\u2014that are encouraging companies to invest in a suite of data technologies. In conversations with data engineers, data scientists, and AI researchers, the need for solutions that can help track data lineage and provenance keeps popping up.\n\nThere are several San Francisco Bay Area companies that have embarked on building data lineage systems\u2014including Salian and his colleagues at Stitch Fix. I wanted to find out how they arrived at the decision to build such a system and what capabilities they are building into it.Continue reading Why companies are in need of data lineage solutions.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lfbPkoveOks/why-companies-are-in-need-of-data-lineage-solutions"
 },
 {
  "title": "Stablecoins: Solving the cryptocurrency volatility crisis",
  "content": "Resolving the volatility problem will unlock the groundwork needed for blockchain-based global payment systems.For cryptocurrency enthusiasts, the long game of blockchain ecosystems is to create open platforms controlled by no single authority, inviting open participation from anyone. This, of course, is in an effort to move forward the culture of open source, from static code branches sitting in source trees to living and evolving useful systems ready for live interaction, still egalitarian and open access in nature.\n\n\n\nAt the heart of this vision lie practically immutable accounting systems that store the widely accepted state of the world to ensure global integrity. Large visions are rarely achieved in one fell swoop and are instead typically realized through the emergence of solutions for related and meaningful short-term problems. The volatility problem is one such important candidate.\n\n\n\nDue to the nascency of cryptocurrency markets, volatility has remained a staple for both outside observers and users. Although traders may fare well from the conditions, those wishing to use the assets in particular and unlock the true value of these accounting systems may have a different experience altogether. An asset bought as a medium of exchange (MoE) may lose its value before any value from spending that asset has truly been captured.\n\n\n\nThe volatility problem is important to solve because its resolution in the short-term unlocks the financial and technical engineering groundwork necessary for cheap, swift, secure, and disintermediated global payment systems based on blockchains. Users of these systems will be able to transact seamlessly with peers at a far cheaper rate and with a dramatic increase of security due to the nature of these cryptographic systems. In the long term, solving this problem will vastly reduce the barrier to entry for use of smart contract-based services on public, consortium, and private blockchains\u2014truly unlocking the automation value these technologies have to offer.\n\n\n\nA cryptocurrency-based contender that has appeared in order to solve this crisis is called a stablecoin. A stablecoin is a cryptocurrency that is pegged by various means to a traditional fiat currency to maintain its price or is backed by precious metals and other commodities. To date, there have been stablecoins backed by fiat currencies in reserve, gold, and even cryptocurrency itself. Although some have failed and lost their desired peg, recent iterations have proven quite successful so far in their stability.\n\nValue opportunity: Digital money\n\nThe digital money industry is booming, driven in recent years especially by the Asia-Pacific region. In 2017, the revenue opportunities exceeded $1.9 trillion, trending double-digit percentage growth per year. While cryptocurrencies have many desirable properties for digital payments, such as low transaction fees, asset transfer simplicity, and auditability, the price volatility of mainstream cryptocurrencies is currently untenable for production use cases. Wild price swings are not good for buying groceries and paying rent. Simple tasks become unreasonably complex, as accounting loses its stability on a multitude of scales.\n\n\n\nFor example, cross-border payments are easy to facilitate with cryptocurrencies but their value fluctuation makes the process unreasonably complicated. If a farmer in an emerging economy wishes to send their family cryptocurrency, they must account for price fluctuations that may devalue the asset at a double-digit percentage before it can be liquidated for a local currency. Stablecoins have the opportunity to reap the benefits of cryptocurrencies while mitigating price volatility to acceptable levels.\n\nValue opportunity: Open platforms\n\nSmart contracting platforms have the potential to solve problems \u201conce and for all,\u201d in the same vein as cloud service providers touting infrastructural block storage (Amazon S3, Google Cloud Storage, Azure Blob) and computing power (Amazon EC2, Google Compute Engine, Azure Compute). Due to their open nature, smart contracts may easily extend to the application layer and offer turnkey solutions to digital money, trade financing, supply chain management, and sharing economies.\n\n\n\nSmart contracts can run at cost without intermediaries carving out high margins, a paradigm similar to well-run public utilities. However, the dominant way to pay for smart contract-based goods and services has been with volatile cryptocurrencies, adding currency risk to businesses that operate on more stable national currencies such as dollars, yuan, or euros. Alternatively, businesses may choose to purchase exact amounts of volatile cryptocurrencies as necessary, minimizing currency exposure but incurring new transaction fees with each subsequent transaction.\n\n\n\nWith the rise of stablecoins, businesses incur smaller currency risks and transaction fees. Due to a guarantee on the price of the asset being processed, there is minimal volatility risk and users and businesses alike can benefit from this stability. This can, in turn, drive adoption for smart contract systems due to reduced risk and improved ease of use. Businesses feel safer due to the benefits of automation and lower transaction costs, and users feel safer in knowing their personal assets won\u2019t be devalued.\n\nStablecoin history\n\nStablecoins aren\u2019t a new revelation in the cryptocurrency space, as attempts at their implementations have been in motion since 2014. The first two attempts at creating stablecoins came from both BitUSD of BitShares, and NuBits\u2014both of which were crypto collateralized iterations. Although both failed due to collateralization instability, they paved the way for new iterations that learned from their mistakes as well as strengthened the case for a price-stable asset represented as a cryptocurrency.\n\nThe first implementation of a reserved back stablecoin came in late 2014 from Tether, which was initially built on Bitcoin through the Omni layer. Although Tether offered the creation and redemption process typically associated with reserve backed stablecoins, its lack of transparency paved the way for transparent reserve backed stablecoins such as CENTRE\u2019s USDC and Gemini\u2019s GUSD.\n\nThere are, in fact, three types of stablecoin implementations: reserve backed, crypto collateralized, and algorithmic, each with its own associated risks and tradeoffs. While reserve backed implementations provide transparency and absolute stability from natural arbitrage opportunities, there is added counterparty risk from the centralized controller. Crypto collateralized implementations forego counterparty risk at the expense of the volatility from their underlying collateral. Algorithmic stablecoins are dependent on incentive mechanisms and speculation, and haven\u2019t had a proven implementation yet. However, all three are interesting in their approaches and allow users to gauge their comfort in using their choice of asset.\n\nStablecoin implementation types\n\nReserve backed\n\nA reserve backed stablecoin is one that is typically issued by a central provider and is backed 1:1 to a fiat currency by means of both a tokenization and redemption process. To generate new tokens, customers send collateral to the provider, and the provider then mints or creates new tokens. The provider typically has the underlying collateral under custody with regular attestation reports, and \u201cburns\u201d or removes the tokens from circulation once they are redeemed.\n\nRecent examples of reserve backed stablecoins include Gemini\u2019s GUSD, CENTRE\u2019s USDC, and Paxos\u2019 PAX. Legacy reserve backed stablecoin Tether (USDT) has suffered much scrutiny due to the provider\u2019s lack of regular auditing of the underlying collateral. However, the latest class of reserve backed stablecoins have been consistently providing audit reports from reputable firms.\n\nReserve backed stablecoins typically retain their peg from arbitrage cycles. For example, if Gemini\u2019s GUSD is trading under $1, arbitrageurs are incentivized to buy the asset until it stabilizes and redeem the tokens for the underlying collateral, thereby making a profit. If the asset is trading over $1, arbitrageurs are incentivized to send collateral to Gemini in order to generate new tokens and sell them for the higher rate.\n\nTypically in these systems, there\u2019s an increased counterparty risk due to the ability of the central provider to freeze assets at any time. This issue came up recently with Gemini customers having redemption issues.\n\nCrypto collateralized\n\nA crypto collateralized stablecoin is one that has cryptocurrency as its underlying collateral and uses price feeds associated with the collateral as a means of keeping the stablecoin at one dollar. In the case of crypto collateralized stablecoins, plenty of projects have made the attempt at stabilizing a system, but none have succeeded as much as MakerDAO.\n\nOriginally launched in 2017, DAI, the crypto collateralized stablecoin from the MakerDAO project, has remained relatively stable with continued development interest. Currently, DAI is collateralized by Ether (ETH), the native currency of the Ethereum blockchain, and the project plans on utilizing other assets for collateral in the future. Users wishing to obtain DAI must first lock ETH in a \u201cCDP,\u201d or collateralized debt position. Due to the high volatility of ETH, DAI is typically overcollateralized at rates well over 100%.\n\nIf an individual\u2019s CDP is ever close to being insolvent, the system triggers the sale of the user\u2019s underlying collateral. If the system ever becomes insolvent due to a market crash in the price of the underlying collateral, Maker or \u201cMKR,\u201d the other token in the MakerDAO system, acts as a buyer of last resort where new MKR tokens are minted, effectively diluting current holders, and sold on the open market as a means of stabilizing the system.\n\nMKR holders are responsible for voting on resolutions and regulating the entire system through MakerDAO\u2019s governance mechanisms. A fee in MKR is also paid to open CDPs to acquire DAI, effectively reducing the supply as more are opened.\n\nCurrently, the key centralized aspect with the most risk in MakerDAO\u2019s system is its oracles that generate price feeds for the underlying collateral. Other centralized actors include \u201ckeepers,\u201d or automated market makers that keep DAI around its target price. Considering limited arbitrage opportunities on a redemption process such as DAIs, external mechanisms such as these must be consulted to maintain stability.\n\nAlgorithmic\n\nThe last major implementation of stablecoins is based on algorithms that trigger supply inflation and deflation in relation to the stablecoin\u2019s target price. Algorithmic stablecoins aren\u2019t backed by collateral, but rather have speculators involved with associated secondary and tertiary assets to keep the system balanced.\n\nAlgorithmic stablecoins are typically reliant on an elastic supply scheme building on Robert Sam\u2019s Seigniorage Shares, where a base stable-asset is produced, but secondary and even tertiary assets generated and redeemed to ensure system stability. Seigniorage generally involves profiting from the creation of currency; the difference in the cost of the production of money and the money itself. In an algorithmic system, speculation on the secondary and tertiary assets to keep an algorithmic system stable is designed to yield a profit. One recent example of an attempt at creating such a system was Basis, a project seeking to build an algorithmic model that recently refunded investors.\n\nThe Basis model included three tokens: a stablecoin to retain its peg at one dollar, and \u201cbonds\u201d and \u201cshares\u201d that act as the secondary and tertiary assets. Bonds are created by the system during periods of price decline under the desired peg and are purchased using the stablecoin to deflate its supply and increase its price. The bonds are immediately redeemed once the price reaches above its target of $1. Shares act as a hedge on a healthy system, as shareholders are granted newly minted stablecoins if all bonds have been redeemed and the stablecoin continues to trade above $1. The inflation mechanism of the stablecoin is meant to drive the price down, as shareholders are granted tokens from it.\n\nTo date, no algorithmic stablecoin has launched successfully. The next attempt at an algorithmic stablecoin launch will likely come from Carbon, and their CUSD stablecoin. At the moment, Carbon is fully collateralized by U.S. dollars held in bank accounts, but they plan on eventually transitioning to an alternative system.\n\nA plethora of implementations\n\nThere is no shortage of projects wishing to issue stablecoins, as it has become quite a hot topic in recent times. To date, there are more than a dozen launched stablecoin projects with more continually being developed to serve different purposes. Most of them have been pegged to the U.S. dollar, but there are a few examples being collateralized by other fiat currencies and commodities. The most notable of which is Digix\u2014which is backed by gold held in a custodial vault. Being able to leverage cost savings internally to an existing ecosystem is one of the primary use cases for institutions and enterprise companies to partner with or create their own stablecoin.\n\nLast year, in particular, saw a dramatic rise in the number of reserved backed stablecoins that launched. Businesses such as Paxos, Gemini, and Circle saw the benefits of launching these assets and have since been building lofty ecosystems around their respective assets. They have also set the standard in regular auditing, providing attestation reports for each asset on a regular basis. This further lowers the risk profile of these assets while providing a safety mechanism to encourage originally skeptic outsiders to finally enter and interface with these technologies.\n\nVolatility mitigation in production\n\nIn order to reap the benefits of stablecoins, decentralized finance (DeFi) services are beginning to accept stablecoins as a means of interfacing with services. DeFi services are those that provide traditional financial services while ensuring transactions are protected, peer to peer, and that users retain custody of their assets at all times. Examples of these services include exchanges, automated asset management funds, and debt platforms.\n\n\n\nOne example of such a service is Dharma \u2014a suite of decentralized lending products. Dharma\u2019s main product involves peer-to-peer lending where users can collateralize their cryptocurrencies in exchange for Ethereum or USDC\u2014a stablecoin. This would allow risk to be averted, as the recipient would be able to avoid market volatility upon receiving a stablecoin. Even BlockFi, a competitor to Dharma, recently announced the integration of GUSD on their platform to help users avoid volatility.\n\n\n\nDecentralized exchanges have also taken note of stablecoins and have continually added them as an available asset for trading. For example, Ethfinex, DDex, and Kyber Network have all listed DAI on their exchanges in order to better assist new traders and those who need to exchange assets and avoid risk after liquidation. Even seasoned traders can now use DAI in order to hedge risk in short-term market fluctuations and benefit from its stability.\n\n\n\nThere are a number of additional projects in the process of launching this year, including MelonPort, an asset management protocol, and Origin, a marketplace generation protocol that will also accept stablecoins as a way to mitigate volatility risk. The increase in stablecoin adoption is a positive signaling mechanism that is demonstrating a natural fit. As more decentralized applications and protocols continue to be developed, stablecoins will naturally serve as a transactional asset to greatly benefit end users.\n\nMass proliferation, or \u201cOne to Rule Them All\u201d\n\nThe recent announcement of the JPM Coin, a stablecoin by J.P. Morgan (which clearly demonstrates an institutional interest in the space), is going to usher in a wave of stablecoins in the next few years. These assets will range from other sovereign-backed fiat currencies such as the Yuan, Yen, and Euro, to other traditional commodities such as precious metals and rare earth minerals. However, their implementations will definitely take note of current iterations that provide a regular price peg free from value fluctuation (if a fiat currency), the least amount of risk for users.\n\n\n\nThe reasoning is that in the short to medium term, stablecoins provide an entrance toward a new financial system. Through this stable onramp into blockchain systems, users need not worry about market volatility when thinking about traditional payments, nor do they need to worry about open platform access being rescinded due to a market downturn. They provide predictability in price and provide a way to sideline funds during market swings without having to pay additional fees moving back into fiat.\n\n\n\nIt is unlikely in the long term that there will be as many stablecoins as are currently available and being developed. While there won\u2019t be a single controlling stablecoin due to the myriad of assets they could be pegged to, there will be a basket that commands a majority of the trading volume. Even with all of its historical issues, Tether still commands a vast majority of trading volume relative to other stablecoins simply because of its social relativity and continued redemption and minting arbitrage opportunities. Competition will drive pricing down and force the technology to grow quickly, but will also result in low or negative margins for the firms producing these but not operating at efficient enough scale.\n\n\n\nThe other inherent risk is to consider if and when cryptocurrencies reach a sufficiently high market capitalization and trading interest. Once enough volume reaches traditional medium of exchange (MoE) cryptocurrencies, prices may begin to plateau and stabilize due to a lack of illiquidity. If Bitcoin were to reach over a sufficient amount of trading volume, having a daily price fluctuation of 10%+ would be highly unlikely, lessening the value proposition of stablecoins. However, stablecoins for the foreseeable future will continue to provide a key stability mechanism by way of the policy and monetary controls we are starting to see implemented in some popular stablecoins today.\n\n\n\nAllowing stablecoins to have monetary controls in place allows not just users but regulators and institutional entities to have a comfortable experience with a traditionally volatile asset class. Enterprises didn\u2019t jump right into cloud computing, and to this day most still have at least some of their processes run locally\u2014this will most likely be the same case for stablecoins and their adoption. Facebook has recently been noted to be working on a stablecoin with their WhatsApp messenger and have been testing it in India. This allows them to govern the monetary policy inside of their apps and build pathways into and out of their ecosystems. Not only does this allow them to save on transaction fees associated with more traditional systems, but it also allows them to guarantee stability in their walled gardens while using cryptocurrency. Facebook being an already digitally native company allows them to test the cost savings aspects while also complying with various regulatory requirements for their users across geographic lines.\n\n\n\nAs the stablecoin ecosystem continues to grow between enterprise players and existing blockchain ecosystem players such as Gemini and Coinbase, users will continue to have the freedom and flexibility to transition between fiat and crypto seamlessly, and more importantly, be granted the stability needed to avoid cryptocurrency\u2019s current volatility crisis.\nContinue reading Stablecoins: Solving the cryptocurrency volatility crisis.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-W4CZU2cC8Y/stablecoins-solving-the-cryptocurrency-volatility-crisis"
 },
 {
  "title": "Four short links: 25 April 2019",
  "content": "Values Risk, Brain Interface, Hacking Scooters, and Behavioral Change\n\nFastly S-1 (SEC) -- Our dedication to our values may negatively influence our financial results. We have taken, and may continue to take, actions that we believe are in the best interests of our customers and our business, even if those actions do not maximize financial results in the short term. For instance, we do not knowingly allow our platform to be used to deliver content from groups that promote violence or hate, and that conflict with our values like strong ethical principles of integrity and trustworthiness, among others. However, this approach may not result in the benefits that we expect or may result in negative publicity, in which case our business could be harmed. (via Anil Dash)\n\nBrain Implant Can Say What You\u2019re Thinking (IEEE Spectrum) -- a new type of BCI, powered by neural networks, that might enable individuals with paralysis or stroke to communicate at the speed of natural speech\u2014an average of 150 words per minute. The technology works via a unique two-step process: first, it translates brain signals into movements of the vocal tract, including the jaw, larynx, lips, and tongue. Second, it synthesizes those movements into speech. The system, which requires a palm-size array of electrodes to be placed directly on the brain, provides a proof of concept that it is possible to reconstruct natural speech from brain activity, the authors say.\n\n\nAustralian Lime Scooters Hacked To Say Sexual Things To Riders -- And while this was just audio files, there have been concerns about scooter hacks that might be more dangerous. Researchers at the security firm Zimperium recently demonstrated that they could force a scooter to accelerate and brake by using a Bluetooth-enabled app from up to 100m away. But Lime doesn\u2019t operate the scooter model that was used in Zimperium\u2019s hack demonstration. Users are hacking scooters around the world to max out their speed and get free rides. But other people are simply interested in adding a little chaos to the world. People have been placing stickers over the QR codes used to start a ride, smashed the scooters in the street, and sometimes simply set them on fire.\n\n\nThe Behavioural Change Stairway Model -- Active Listening; Empathy; Rapport; Influence; Behavioural Change. [...] Though the stakes of business negotiations are usually not as high as that of a hostage negotiation, the psychological basis for diffusing conflict are related between the two contexts. The manager who is negotiating with a frustrated employee or client will be well served by walking with his or her counterpart up the \u201cBehavioral Change Stairway.\u201d. (via Simon Willison)\n\nContinue reading Four short links: 25 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/_hcDgO7JMhU/four-short-links-25-april-2019"
 },
 {
  "title": "Four short links: 24 April 2019",
  "content": "Control is a Shrug, Glitch Languages, Streaming Media Server, and CRISPR's New Model Organisms\n\nUsers Want Control is a Shrug (Ian Bicking) -- Making the claim \u201cusers want control\u201d is the same as saying you don\u2019t know what users want, you don\u2019t know what is good, and you don\u2019t know what their goals are.\n\n\nLanguage Support on Glitch: A List --a write-up of getting languages running in the Glitch environment. (via Simon Willison)\n\nAnt Media Server -- open source streaming media server, supports RTMP, RTSP, WebRTC, and Adaptive Bitrate. It can also record videos in MP4, HLS, and FLV.\n\n\nCRISPR Gene-editing Creates Wave of Exotic Model Organisms (Nature) -- Biologists have embraced CRISPR\u2019s ability to quickly and cheaply modify the genomes of popular model organisms, such as mice, fruit flies, and monkeys. Now they are trying the tool on more-exotic species, many of which have never been reared in a lab or had their genomes analyzed. \u201cWe finally are ready to start expanding what we call a model organism,\u201d says Tessa Montague, a molecular biologist at Columbia University in New York City.\n\n\nContinue reading Four short links: 24 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/efv94a-yfNs/four-short-links-24-april-2019"
 },
 {
  "title": "Four short links: 23 April 2019",
  "content": "Worker-run Gig Factories, Persistence of Firefighting, Discriminating Systems, and Activation Atlas\n\nWhen Workers Control the Code (Wired) -- workers form co-ops to code and run gig economy apps, and make decent rates because there's no rent-seeker platform in the middle. A great counter for rising prices and plummeting driver pay post-IPO. (via BoingBoing) \n\nThe Persistence of Firefighting in Product Development -- The most important result of our studies is that product development systems have a tipping point. In models of infectious diseases, the tipping point represents the threshold of infectivity and susceptibility beyond which a disease becomes an epidemic. Similarly, in product development systems there exists a threshold for problem-solving activity that, when crossed, causes firefighting to spread rapidly from a few isolated projects to the entire development system. Our analysis also shows that the location of the tipping point, and therefore the susceptibility of the system to the firefighting phenomenon, is determined by resource utilization in steady state.\n\n\nDiscriminating Systems -- headlines from the major findings: There is a diversity crisis in the AI sector across gender and race. The AI sector needs a profound shift in how it addresses the current diversity crisis. The overwhelming focus on \"women in tech\" is too narrow and likely to privilege white women over others. Fixing the \"pipeline\" won\u2019t fix AI\u2019s diversity problems. The use of AI systems for the classification, detection, and prediction of race and gender is in urgent need of re-evaluation. Also comes with recommendations.\n\nActivation Atlas -- By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned which can reveal how the network typically represents some concepts. Beautiful.\n\nContinue reading Four short links: 23 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zGjHWj4W7j4/four-short-links-23-april-2019"
 },
 {
  "title": "Four short links: 22 April 2019",
  "content": "GANs via Spreadsheet, Open Source Chat, Sandboxing Libraries, and Flat Robot Sales\n\nSpacesheet -- Interactive Latent Space Exploration through a Spreadsheet Interface. (via Flowing Data)\n\nTchap -- the French government's open source secure encrypted chat tool, built off the open source Riot. (via ZDNet)\n\nSandboxed API -- Google open-sourced their tool for automatically generating sandboxes for C/C++ libraries. (via Google Blog)\n\nIndustrial Robot Sales Flat (Robohub) -- It was only up 1% over 2017. Important note: No information was given about service and field robotics. (which may well be booming)\n\nContinue reading Four short links: 22 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/s-AJctP8Q4Y/four-short-links-22-april-2019"
 },
 {
  "title": "Four short links: 19 April 2019",
  "content": "AI Music, Mind-Controlled Robot Hands, Uber's Repo Tools, and Career Resilience\n\nAI and Music (The Verge) -- total legal clusterf*ck.\n\nA Robot Hand Controlled with the Mind -- student uses open source hand and trains brain-machine interface, and holy crap we live in an age when these kinds of things are relatively easy to do rather than requiring massive resources.\n\nKeeping Master Green -- This paper presents the design and implementation of SubmitQueue. It guarantees an always green master branch at scale: all build steps (e.g., compilation, unit tests, UI tests) successfully execute for every commit point. SubmitQueue has been in production for over a year and can scale to thousands of daily commits to giant monolithic repositories. Uber's tech. (via Adrian Colyer)\n\nEarly Career Setback and Future Career Impact -- Our analyses reveal that an early-career near miss has powerful, opposing effects. On one hand, it significantly increases attrition, with one near miss predicting more than a 10% chance of disappearing permanently from the NIH system. Yet, despite an early setback, individuals with near misses systematically outperformed those with near wins in the longer run, as their publications in the next 10 years garnered substantially higher impact. We further find that this performance advantage seems to go beyond a screening mechanism, whereby a more selected fraction of near-miss applicants remained than the near winners, suggesting that early-career setback appears to cause a performance improvement among those who persevere. Overall, the findings are consistent with the concept that \"what doesn't kill me makes me stronger.\"\n\n\nContinue reading Four short links: 19 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jyeNZnp4PtY/four-short-links-19-april-2019"
 },
 {
  "title": "Software 2.0 and Snorkel",
  "content": "Christopher R\u00e9 discusses Snorkel, a system for fast training data creation.Continue reading Software 2.0 and Snorkel.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/oO3vcuaxmbw/software-2-and-snorkel"
 },
 {
  "title": "Automation of AI: Accelerating the AI revolution",
  "content": "Ruchir Puri discusses the next revolution in automating AI, which strives to deploy AI to automate the task of building, deploying, and managing AI tasks.Continue reading Automation of AI: Accelerating the AI revolution.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VqHzvfsimTs/automation-of-ai-accelerating-the-ai-revolution"
 },
 {
  "title": "Applied machine learning at Facebook",
  "content": "Kim Hazelwood discusses the hardware and software Facebook has designed to meet its scale needs.Continue reading Applied machine learning at Facebook.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/IrE7Vt3LZ8M/applied-machine-learning-at-facebook"
 },
 {
  "title": "Computational propaganda",
  "content": "Sean Gourley considers the repercussions of AI-generated content that blurs the line between what's real and what's fake.Continue reading Computational propaganda.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FDP6d7oHhaE/computational-propaganda"
 },
 {
  "title": "Decoding the human genome with deep learning",
  "content": "How can machine learning decode the mysteries of life? Olga Troyanskaya explores this and other big questions through the prism of deep learning.Continue reading Decoding the human genome with deep learning.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gpPn55LXv40/decoding-the-human-genome-with-deep-learning"
 },
 {
  "title": "Simple, scalable, and sustainable: A methodical approach to AI adoption",
  "content": "Rajendra Prasad explains how leaders in large enterprises can make AI adoption successful.Continue reading Simple, scalable, and sustainable: A methodical approach to AI adoption.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3zk1EfFXffk/simple-scalable-and-sustainable-a-methodical-approach-to-ai-adoption"
 },
 {
  "title": "Artificial intelligence: The \u201crefinery\u201d for data",
  "content": "Nick Curcuru explains how Mastercard is using AI to improve security without sacrificing the customer experience.Continue reading Artificial intelligence: The \u201crefinery\u201d for data.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/iYN-NWfLr_Y/artificial-intelligence-the-refinery-for-data"
 },
 {
  "title": "Making real-world distributed deep learning easy with Nauta",
  "content": "Carlos Humberto Morales offers an overview of Nauta, an open source multiuser platform that lets data scientists run complex deep learning models on shared hardware.Continue reading Making real-world distributed deep learning easy with Nauta.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/W4fWmV8TaIo/making-real-world-distributed-deep-learning-easy-with-nauta"
 },
 {
  "title": "Four short links: 18 April 2019",
  "content": "Geospatial Feature Engineering, 3D Reconstruction, Fast NLP, and Learning the Zork Interpreter Language\n\nGeomancer -- a geospatial feature engineering library. It leverages geospatial data such as OpenStreetMap (OSM) alongside a data warehouse like BigQuery. You can use this to create, share, and iterate geospatial features for your downstream tasks (analysis, modeling, visualization, etc.).\n\n\nMeshroom -- a free, open source 3D Reconstruction Software based on the AliceVision framework.\n\n\nBlingFire -- A lightning fast finite state machine and regular expression manipulation library. [...] We use Fire for many linguistic operations inside Bing such as tokenization, multi-word expression matching, unknown word-guessing, stemming / lemmatization, just to mention a few. cf NLTK.\n\nLearning ZIL -- what the Infocom games were written in, decades before Inform. Andrew Plotkin wrote an intro that explains how it sits in the universe. (Note: this is useless but historically interesting.)\n\nContinue reading Four short links: 18 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Is6c32TaifE/four-short-links-18-april-2019"
 },
 {
  "title": "Automated ML: A journey from CRISPR.ML to Azure ML",
  "content": "Danielle Dean explains how cloud, data, and AI came together to help build Automated ML.Continue reading Automated ML: A journey from CRISPR.ML to Azure ML .",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/_w8viElzqVE/automated-ml-a-journey-from-crisperml-to-azure-ml"
 },
 {
  "title": "Toward ethical AI: Inclusivity as a messy, difficult, but promising answer",
  "content": "Kurt Muehmel explores AI within a broader discussion of the ethics of technology, arguing that inclusivity and collaboration are necessary.Continue reading Toward ethical AI: Inclusivity as a messy, difficult, but promising answer.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/G-2Vpjrlwqk/toward-ethical-ai"
 },
 {
  "title": "Machine learning for personalization",
  "content": "Tony Jebara explains how Netflix is personalizing and optimizing the images shown to subscribers.Continue reading Machine learning for personalization.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ECfiz2ewYcM/machine-learning-for-personalization"
 },
 {
  "title": "Is AI human-ready?",
  "content": "Aleksander Madry discusses roadblocks preventing AI from having a broad impact and approaches for addressing these issues.Continue reading Is AI human-ready?.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/J8n7f1GeJ6o/is-ai-human-ready"
 },
 {
  "title": "Checking in on AI tools",
  "content": "Ben Lorica and Roger Chen assess the state of AI technologies and adoption in 2019.Continue reading Checking in on AI tools.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YD1AbXsCUjo/checking-in-on-ai-tools"
 },
 {
  "title": "Data fueling AI of the future",
  "content": "Thomas Henson considers how AI will shape the experiences of future generations.Continue reading Data fueling AI of the future.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/bAasRFtMV1A/data-fueling-ai-of-the-future"
 },
 {
  "title": "Highlights from the O'Reilly Artificial Intelligence Conference in New York 2019",
  "content": "Watch highlights from expert talks covering AI, machine learning, deep learning, ethics, and more.People from across the AI world came together in New York for the O'Reilly Artificial Intelligence Conference. Below you'll find links to highlights from the event.\n\nAI and the robotics revolution\n\nMartial Hebert offers an overview of challenges in AI for robotics and a glimpse at the exciting developments emerging from current research.\n\n\n\tWatch \"AI and the robotics revolution.\"\n\n\nApplied machine learning at Facebook\n\nKim Hazelwood discusses the hardware and software Facebook has designed to meet its scale needs.\n\n\n\tWatch \"Applied machine learning at Facebook.\"\n\n\nDecoding the human genome with deep learning\n\nHow can machine learning decode the mysteries of life? Olga Troyanskaya explores this and other big questions through the prism of deep learning.\n\n\n\tWatch \"Decoding the human genome with deep learning.\"\n\n\nComputational propaganda\n\nSean Gourley considers the repercussions of AI-generated content that blurs the line between what's real and what's fake.\n\n\n\tWatch \"Computational propaganda.\"\n\n\nSoftware 2.0 and Snorkel\n\nChristopher R\u00e9 discusses Snorkel, a system for fast training data creation.\n\n\n\tWatch \"Software 2.0 and Snorkel.\"\n\n\nIs AI human-ready?\n\nAleksander Madry discusses roadblocks preventing AI from having a broad impact and approaches for addressing these issues.\n\n\n\tWatch \"Is AI human-ready?\"\n\n\nMachine learning for personalization\n\nTony Jebara explains how Netflix is personalizing and optimizing the images shown to subscribers.\n\n\n\tWatch \"Machine learning for personalization.\"\n\n\nChecking in on AI tools\n\nBen Lorica and Roger Chen assess the state of AI technologies and adoption in 2019.\n\n\n\tWatch \"Checking in on AI tools.\"\n\n\nFast, flexible, and functional: 4 real-world AI deployments at enterprise scale\n\nGadi Singer discusses the major questions organizations confront as they integrate deep learning.\n\n\n\tWatch \"Fast, flexible, and functional: 4 real-world AI deployments at enterprise scale.\"\n\n\nMaking real-world distributed deep learning easy with Nauta\n\nCarlos Humberto Morales offers an overview of Nauta, an open source multiuser platform that lets data scientists run complex deep learning models on shared hardware.\n\n\n\tWatch \"Making real-world distributed deep learning easy with Nauta.\"\n\n\nAutomated ML: A journey from CRISPR.ML to Azure ML\n\nDanielle Dean explains how cloud, data, and AI came together to help build Automated ML.\n\n\n\tWatch \"Automated ML: A journey from CRISPR.ML to Azure ML.\"\n\n\nToward ethical AI: Inclusivity as a messy, difficult, but promising answer\n\nKurt Muehmel explores AI within a broader discussion of the ethics of technology, arguing that inclusivity and collaboration are necessary.\n\n\n\tWatch \"Toward ethical AI: Inclusivity as a messy, difficult, but promising answer.\"\n\n\nHow AI adaptive technology can upgrade education\n\nJoleen Liang explains how AI and precise knowledge points can help students learn.\n\n\n\tWatch \"How AI adaptive technology can upgrade education.\"\n\n\nArtificial intelligence: The \u201crefinery\u201d for data\n\nNick Curcuru explains how Mastercard is using AI to improve security without sacrificing the customer experience.\n\n\n\tWatch \"Artificial intelligence: The 'refinery' for data.\"\n\n\nAutomation of AI: Accelerating the AI revolution\n\nRuchir Puri discusses the next revolution in automating AI, which strives to deploy AI to automate the task of building, deploying, and managing AI tasks.\n\n\n\tWatch \"Automation of AI: Accelerating the AI revolution.\"\n\n\nSimple, scalable, and sustainable: A methodical approach to AI adoption\n\nRajendra Prasad explains how leaders in large enterprises can make AI adoption successful.\n\n\n\tWatch \"Simple, scalable, and sustainable: A methodical approach to AI adoption.\"\n\n\nData fueling AI of the future\n\nThomas Henson considers how AI will shape the experiences of future generations.\n\n\n\tWatch \"Data fueling AI of the future.\"\n\n\nContinue reading Highlights from the O'Reilly Artificial Intelligence Conference in New York 2019 .",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0-EZhzTZPFg/highlights-from-oreilly-ai-ny-2019"
 },
 {
  "title": "How AI adaptive technology can upgrade education",
  "content": "Joleen Liang explains how AI and precise knowledge points can help students learn.Continue reading How AI adaptive technology can upgrade education.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vfTCakCz8HY/how-ai-adaptive-technology-can-upgrade-education"
 }
]